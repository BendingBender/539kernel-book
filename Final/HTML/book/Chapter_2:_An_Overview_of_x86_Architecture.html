<!-- The Template is Based on: https://github.com/ryangrose/easy-pandoc-templates -->
<!doctype html>
<html >
<head>
    
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />
        <!--[if lt IE 9]>
                <script src="http://css3-mediaqueries-js.googlecode.com/svn/trunk/css3-mediaqueries.js"></script>
        <![endif]-->
    <meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
    <meta http-equiv="Content-Style-Type" content="text/css" />

    <link href="https://fonts.googleapis.com/css?family=Domine|Montserrat" rel="stylesheet"> 
    <link href="https://stackpath.bootstrapcdn.com/bootstrap/4.1.1/css/bootstrap.min.css" rel="stylesheet" integrity="sha384-WskhaSGFgHYWDcbwN70/dfYBj47jz9qbsMId/iRN3ewGhXQFZCSftd1LZCfmhktB" crossorigin="anonymous">
    <script src="https://stackpath.bootstrapcdn.com/bootstrap/4.1.1/js/bootstrap.min.js" integrity="sha384-smHYKdLADwkXOn1EmN1qk/HfnUcbVRZyYmZ4qpPea6sjB/pTJ0euyQp0Mk8ck+5T" crossorigin="anonymous"></script>
    <!-- <link rel="stylesheet" type="text/css" href="template.css" /> -->
    <link rel="stylesheet" type="text/css" href="https://cdn.jsdelivr.net/gh/diversen/pandoc-bootstrap-adaptive-template@959c3622/template.css" />

    <link href="https://vjs.zencdn.net/5.4.4/video-js.css" rel="stylesheet" />

    <script src="https://code.jquery.com/jquery-2.2.1.min.js"></script>
    <!-- <script type='text/javascript' src='menu/js/jquery.cookie.js'></script> -->
    <!-- <script type='text/javascript' src='menu/js/jquery.hoverIntent.minified.js'></script> -->
    <!-- <script type='text/javascript' src='menu/js/jquery.dcjqaccordion.2.7.min.js'></script> -->
    <!-- <link href="menu/css/skins/blue.css" rel="stylesheet" type="text/css" /> -->
    <!-- <link href="menu/css/skins/graphite.css" rel="stylesheet" type="text/css" /> -->
    <!-- <link href="menu/css/skins/grey.css" rel="stylesheet" type="text/css" /> -->
    <!-- <script src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script> -->
    <!-- <script src="script.js"></script> -->
    <!-- <script src="jquery.sticky-kit.js "></script> -->
    <script type='text/javascript' src='https://cdn.jsdelivr.net/gh/diversen/pandoc-bootstrap-adaptive-template@959c3622/menu/js/jquery.cookie.js'></script>
    <script type='text/javascript' src='https://cdn.jsdelivr.net/gh/diversen/pandoc-bootstrap-adaptive-template@959c3622/menu/js/jquery.hoverIntent.minified.js'></script>
    <script type='text/javascript' src='https://cdn.jsdelivr.net/gh/diversen/pandoc-bootstrap-adaptive-template@959c3622/menu/js/jquery.dcjqaccordion.2.7.min.js'></script>

    <link href="https://cdn.jsdelivr.net/gh/diversen/pandoc-bootstrap-adaptive-template@959c3622/menu/css/skins/blue.css" rel="stylesheet" type="text/css" />
    <!--
    <link href="https://cdn.jsdelivr.net/gh/diversen/pandoc-bootstrap-adaptive-template@959c3622/menu/css/skins/graphite.css" rel="stylesheet" type="text/css" />
    <link href="https://cdn.jsdelivr.net/gh/diversen/pandoc-bootstrap-adaptive-template@959c3622/menu/css/skins/grey.css" rel="stylesheet" type="text/css" />
    -->
    <link href="https://cdn.jsdelivr.net/gh/ryangrose/easy-pandoc-templates@948e28e5/css/elegant_bootstrap.css" rel="stylesheet" type="text/css" />
  
    <script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.4/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>
  
    <script src="https://cdn.jsdelivr.net/gh/diversen/pandoc-bootstrap-adaptive-template@959c3622/script.js"></script>
  
    <script src="https://cdn.jsdelivr.net/gh/diversen/pandoc-bootstrap-adaptive-template@959c3622/jquery.sticky-kit.js"></script>
    <meta name="generator" content="pandoc" />
  	<meta name="author" content="Mohammed Q. Hussain" />
  <title>Chapter 2: An Overview of x86 Architecture</title>
  <style type="text/css">code{white-space: pre;}</style>
  <style type="text/css">body { font-family: Domine, Georgia, Palatino, 'Palatino Linotype', Times, 'Times New Roman', serif} h1, h2, h3, h4, h5, h6 { font-family: Montserrat, sans }</style>
  <style type="text/css">
div.sourceCode { overflow-x: auto; }
table.sourceCode, tr.sourceCode, td.lineNumbers, td.sourceCode {
  margin: 0; padding: 0; vertical-align: baseline; border: none; }
table.sourceCode { width: 100%; line-height: 100%; }
td.lineNumbers { text-align: right; padding-right: 4px; padding-left: 4px; color: #aaaaaa; border-right: 1px solid #aaaaaa; }
td.sourceCode { padding-left: 5px; }
code > span.kw { color: #007020; font-weight: bold; } /* Keyword */
code > span.dt { color: #902000; } /* DataType */
code > span.dv { color: #40a070; } /* DecVal */
code > span.bn { color: #40a070; } /* BaseN */
code > span.fl { color: #40a070; } /* Float */
code > span.ch { color: #4070a0; } /* Char */
code > span.st { color: #4070a0; } /* String */
code > span.co { color: #60a0b0; font-style: italic; } /* Comment */
code > span.ot { color: #007020; } /* Other */
code > span.al { color: #ff0000; font-weight: bold; } /* Alert */
code > span.fu { color: #06287e; } /* Function */
code > span.er { color: #ff0000; font-weight: bold; } /* Error */
code > span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
code > span.cn { color: #880000; } /* Constant */
code > span.sc { color: #4070a0; } /* SpecialChar */
code > span.vs { color: #4070a0; } /* VerbatimString */
code > span.ss { color: #bb6688; } /* SpecialString */
code > span.im { } /* Import */
code > span.va { color: #19177c; } /* Variable */
code > span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code > span.op { color: #666666; } /* Operator */
code > span.bu { } /* BuiltIn */
code > span.ex { } /* Extension */
code > span.pp { color: #bc7a00; } /* Preprocessor */
code > span.at { color: #7d9029; } /* Attribute */
code > span.do { color: #ba2121; font-style: italic; } /* Documentation */
code > span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code > span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code > span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */

	/* [MQH] */
	.toc a { color: #333 !important; }
	h1 { margin-bottom: 5% }
	body { font-size: 15pt; !important }
  </style>
</head>

<body>

    
    <div class="navbar navbar-static-top navbar-expand-sm px-0 pt-0">
    <div class="navbar-inner container-fluid">
      <div class="container">
        <span class="doc-title">A Journey in Creating an Operating System Kernel: The 539kernel Book</span>
                <span class="doc-title"><small>Mohammed Q. Hussain</small></span>
                <ul class="nav pull-right doc-info">
          <li><p class="navbar-text"><a href="index.html" style="color: #777777; !important">Table of Content</a></li>
                  </ul>
      </div>
    </div>
  </div>
  <div class="container">
      <div class="row">
            <div class="col-12">

      
      <h1 id="ch-"x86">Chapter <span class="header-section-number">2</span>. An Overview of x86 Architecture</h1>
<h2 id="introduction"><span class="header-section-number">2.1</span> Introduction</h2>
<p>In our situation, and by using modern terminology, we can view the processor as a <em>library</em> and <em>framework</em>. A library because it provides us with a bunch of instructions to perform whatever we want, and a framework because it has general rules that organize the overall environment of execution, that is, it forces us to work in a specific way. We have seen some aspects of the first part when we have written the bootloader, that is, we have seen the processor as a library. In this chapter, we are going to see how the processor works as a framework by examining some important and basic concepts of x86. We need to understand these concepts to start the real work of writing 539kernel.</p>
<h2 id="x86-operating-modes"><span class="header-section-number">2.2</span> x86 Operating Modes</h2>
<p>In x86 an operating mode specifies the overall picture of the processor, such as the maximum size of available registers, the available advanced features for the running operating system, the restrictions and so on.</p>
<p>When we developed the bootloader in the previous chapter we have worked with an x86 operating mode named <em>real address mode</em> (or for short <em>real mode</em>) which is an old operating mode that is still supported by modern x86 processors for the sake of backward compatibility, due to that when the computer is turned on, it initially runs on real mode.</p>
<p>Real mode is a <code>16-bit</code> operating mode which means that, maximally, only <code>16</code> bits of register size can be used, even if the actual size of the registers is 64-bit. Using only <code>16</code> bits of registers has consequences other than the size itself, these consequences are considered as disadvantages in modern days, for example, in real mode the size of the main memory is limited, even if the computer has 16GB of memory, real mode can deal only with 1MB. Furthermore, any code which runs on real mode should be a <code>16-bit</code> code, for example, the aforementioned <code>32-bit</code> registers (such as <code>eax</code>) cannot be used in real mode code, their <code>16-bit</code> counterparts should be used instead, as an example, the 16-bit <code>ax</code> should be used instead of <code>eax</code> and so on.</p>
<p>Some core features of modern operating systems nowadays are: multitasking, memory protection and virtual memory <a href="#fn1" class="footnoteRef" id="fnref1"><sup>1</sup></a> and real mode provides nothing to implement these features. However, in modern x86 processors, new and more advanced operating modes have been introduced, namely, <em>protected mode</em> which is a <code>32-bit</code> operating mode and <em>long mode</em> which is a relatively new <code>64-bit</code> operating mode. Although the long mode provides more capacity for its users, for example, it can deal with 16 <strong>exabytes</strong> of memory, we are going to focus on protected mode since it provides the same basic mechanisms that we need to develop a modern operating system kernel with the aforementioned features, hence, 539kernel is a <code>32-bit</code> kernel which runs under protected mode.</p>
<p>Since protected mode is a <code>32-bit</code> operating mode then <code>32</code> bits of registers can be used, also, protected mode has the ability to deal with <code>4GB</code> of main memory, and most importantly, it provides important features which we are going to explore through this book that helps us in implementing modern operating system kernel features.</p>
<p>As we have said before, <em>multitasking</em> is one of core features that modern operating systems provide. In multitasking environment more than one software can run at the same time, at least illusionary, even if there is only one processor or the current processor has only one core. For the sake of making our next discussion easier we should define the term <em>process</em> which means a program that is currently running. For example, if your web browser is currently running then this running instance of it is called a process, its code is loaded into the main memory and the processor is currently executing it. Another property of general-purpose operating systems is that they allow the user to run any software from unknown sources which means that these software cannot be trusted, they may contain code that intentionally or even unintentionally breach the security of the system or cause the system to crash.</p>
<p>Due to these two properties of modern general-purpose operating systems, the overall system needs to be protected from multiple actions. <strong>First</strong>, either in multitasking or monotasking <a href="#fn2" class="footnoteRef" id="fnref2"><sup>2</sup></a> environment, the kernel of the operating system which is the most sensitive part of the system, should be protected from current processes, no process should be able to access any part of the kernel's memory either by reading from it or writing to it, also, no process should be able to call any of kernel's code without kernel's consent. <strong>Second</strong>, the sensitive instructions and registers that change the behavior of the processor (e.g. switching from real-mode to protected-mode) should be only allowed for the kernel which is the most privileged component of the system, otherwise, the stability of the system will be in danger. <strong>Third</strong>, in multitasking environment the running processes should be protected from each other in the same way the kernel is protected from them, no process should interfere with another.</p>
<p>In x86, the <em>segmentation</em> mechanism provided a logical view of memory in real-mode and it has been extended in protected-mode to provide the needed protection which has been described in the third point <a href="#fn3" class="footnoteRef" id="fnref3"><sup>3</sup></a>, while segmentation can be used in x86 for this kind of protection, it is not the sole way to perform that, the another well-known way is called <em>paging</em>, but segmentation is the default in x86 and cannot be turned off while paging is optional, the operating system has the option to use it as a way of memory protection or not.</p>
<p>Also, in the protected-mode the concept of <em>privilege levels</em> has been introduced to handle the protections needed in the previous first and second points. The academic literature of operating systems separate the system environment into two modes, <em>kernel-mode</em> and <em>user-mode</em>, at a given time, the system may run on one of these modes and not both of them. The kernel runs on kernel-mode, and has the privilege to do anything (e.g. access any memory location, access any resource of the system and perform any operation), while the user applications run on user-mode which is a restricted environment where the code that runs on doesn't have the privilege to perform sensitive actions.</p>
<p>This kind of separation has been realized through privilege levels in x86 which provides <strong>four privilege levels</strong> numbered from <code>0</code> to <code>3</code>. The privilege level <code>0</code> is the most privileged level which can be used to realize the kernel-mode while the privilege level <code>3</code> is the least privileged which can be used to realize the user-mode. For privilege levels <code>1</code> and <code>2</code> it's up to the kernel's designer to use them or not, some kernel designs suggest to use these levels for device drivers. According to Intel's manual, if the kernel's design uses only two privilege levels, that is, a kernel-mode and user-mode, then the privilege level <code>0</code> and <code>3</code> should be used and not for example <code>0</code> and <code>1</code>.</p>
<p>In addition to protecting the kernel's code from being called without its consent and protecting kernel's data from being accessed by user processes (as both required by first point above), the privilege levels also prevent user processes <a href="#fn4" class="footnoteRef" id="fnref4"><sup>4</sup></a> from executing <em>privileged instructions</em> (as required by second point above), only the code which runs in the privilege level <code>0</code>, that is, the kernel, will be able to execute these instructions since they could manipulate sensitive parts of the processor's environment <a href="#fn5" class="footnoteRef" id="fnref5"><sup>5</sup></a> that only the kernel should maintain.</p>
<p>When a system uses different privilege levels to run, as in most modern operating systems, the x86 processor maintains what is called <em>current privilege level</em> (CPL) which is, as its name suggests, the current privilege level of the currently running code. For example, if the currently running code belongs to the kernel then the current privilege level will be <code>0</code> and according to it, the processor is going to decide allowed operations. In other words, we can say that the processor keeps tracking the current state of the currently running system and one of the information in this state is in which privilege level (or mode) the system is currently running.</p>
<h2 id="numbering-systems"><span class="header-section-number">2.3</span> Numbering Systems</h2>
<p>The processor works with all values as <em>binary numbers</em> while it is natural for us as human beings to deal with numbers as <em>decimal numbers</em>. A number by itself is an abstract concept, it is something in our mind, but to communicate with each others, we represent the numbers by using symbols which is named <em>numerals</em>. For example, the conceptual number <code>one</code> can be represented by different <em>numeral systems</em>. In Arabic numeral system the number <code>one</code> is expressed as <code>1</code>, while in Roman numeral system it is expressed as <code>I</code>.</p>
<p>A numeral system is <em>writing system</em>, that is, it gives us rules of how to write a number down as a symbol, it focuses on the way of writing the numbers. On the other hand, the numbers can be dealt with by a <em>numbering system</em>, we use the <em>decimal numbering system</em> to deal with numbers, think about them and perform arithmetic operations upon them, the processor uses the <em>binary numbering system</em> to do the same with numbers. There are numbering systems other than the decimal and binary numbering system, and any number can be represented by any numbering system.</p>
<p>A number system is defined by its <em>base</em> which is also called <em>radix</em>, this base defines the list of available <em>digits</em> in the numbering system starting from <code>0</code> to <code>base - 1</code>, the total of available digits equals the base. Consider the decimal numbering system, its base is <code>10</code> which means the available digits in this system are: <code>0, 1, 2, 3, 4, 5, 6, 7, 8, 9</code>, a total of <code>10</code> digits. These digits can be used to create larger numbers, for example, <code>539</code> which consists of the digits <code>5</code>, <code>3</code> and <code>9</code>.</p>
<p>On the other hand, the base of binary numbering system is <code>2</code>, therefore, the available digits are only <code>0</code> and <code>1</code>, and as in the decimal numbering system they can be used to compose larger numbers, for example, the number <code>two</code> in binary numbering system is <code>10</code> <a href="#fn6" class="footnoteRef" id="fnref6"><sup>6</sup></a>, be careful, this numeral does not represent the number <code>ten</code>, it represents the number <code>two</code> but in binary numbering system. When we discuss numbers in different numbering systems, we put the initial letter of the numbering system name in the end of the number, for example, <code>10d</code> and <code>10b</code> are two different numbers, the first one is <code>ten</code> in <strong>d</strong>ecimal while the second one is <code>two</code> in <strong>b</strong>inary.</p>
<p>Furthermore, basic arithmetic operations such as addition and subtraction can be performed on the numbering system, for example, in binary <code>1 + 1 = 10</code> and it can be performed systematically, also, a representation of any number in any numbering system can be converted to any other numbering system systematically <a href="#fn7" class="footnoteRef" id="fnref7"><sup>7</sup></a>, while this is not a good place to show how to perform the operations and conversions for different numbering system, you can find many online resources that explain these operations on the well-known number systems.</p>
<p>By now it should be obvious for you that changing the base (radix) gives us a new numbering system and the base can be any number which implies that the total of numbering systems is infinite! One of useful and well-known numbering system is <em>hexadecimal</em> which its base is <code>16</code> and its digits are <code>0, 1, 2, 3, 4, 5, 6, 7, 8, 9, A, B, C, D, E, F</code> where <code>A</code> means <code>ten</code>, <code>B</code> means <code>eleven</code> and so on. So, why hexadecimal is useful in our situation? Since binary is used in the processor it will be easier to discuss some related entities such as the value of <code>FLAGS</code> register which each bit on it represents a value for a different thing, another example is memory addresses. But consider the following example which is a binary number that represents a memory address.</p>
<div class="sourceCode"><pre class="sourceCode asm"><code class="sourceCode fasm"><span class="dv">00000000</span> <span class="dv">00000000</span> <span class="dv">00000000</span><span class="bn"> 00000001b</span></code></pre></div>
<p>It is too long and it will be tedious to work with, and for that the hexadecimal numbering system can be useful. Each digit in hexadecimal represents <strong>four</strong> bits <a href="#fn8" class="footnoteRef" id="fnref8"><sup>8</sup></a>, that is, the number <code>0h</code> in <strong>h</strong>exadecimal is equivalent to <code>0000b</code> in binary. As the <code>8 bits</code> known as a byte, the <code>4 bits</code> is known as a <em>nibble</em>, that is, a nibble is a half byte and, as we have said, but in other words, one digit of hexadecimal represents a nibble. So, we can use hexadecimal to represent the same memory address value in more elegant way.</p>
<div class="sourceCode"><pre class="sourceCode asm"><code class="sourceCode fasm"><span class="dv">00</span> <span class="dv">00</span> <span class="dv">00</span><span class="bn"> 01h</span></code></pre></div>
<table>
<caption>An Example of How Zero to Fifteen are Represented in the Three Numbering Systems.</caption>
<thead>
<tr class="header">
<th>Decimal</th>
<th>Binary</th>
<th>Hexadecimal</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>0</td>
<td>0</td>
<td>0</td>
</tr>
<tr class="even">
<td>1</td>
<td>1</td>
<td>1</td>
</tr>
<tr class="odd">
<td>2</td>
<td>10</td>
<td>2</td>
</tr>
<tr class="even">
<td>3</td>
<td>11</td>
<td>3</td>
</tr>
<tr class="odd">
<td>4</td>
<td>100</td>
<td>4</td>
</tr>
<tr class="even">
<td>5</td>
<td>101</td>
<td>5</td>
</tr>
<tr class="odd">
<td>6</td>
<td>110</td>
<td>6</td>
</tr>
<tr class="even">
<td>7</td>
<td>111</td>
<td>7</td>
</tr>
<tr class="odd">
<td>8</td>
<td>1000</td>
<td>8</td>
</tr>
<tr class="even">
<td>9</td>
<td>1001</td>
<td>9</td>
</tr>
<tr class="odd">
<td>10</td>
<td>1010</td>
<td>A</td>
</tr>
<tr class="even">
<td>11</td>
<td>1011</td>
<td>B</td>
</tr>
<tr class="odd">
<td>12</td>
<td>1100</td>
<td>C</td>
</tr>
<tr class="even">
<td>13</td>
<td>1101</td>
<td>D</td>
</tr>
<tr class="odd">
<td>14</td>
<td>1110</td>
<td>E</td>
</tr>
<tr class="even">
<td>15</td>
<td>1111</td>
<td>F</td>
</tr>
</tbody>
</table>
<h2 id="the-basic-view-of-memory"><span class="header-section-number">2.4</span> The Basic View of Memory</h2>
<p>The basic view of the main memory is that it is an <em>array of cells</em>, each cell has the ability to store one byte and it is reachable by a unique number called <em>memory address</em> <a href="#fn9" class="footnoteRef" id="fnref9"><sup>9</sup></a>, the range of memory addresses starts from <code>0</code> to some limit <code>x</code>, for example, if the system has <code>1MB</code> of <em>physical</em> main memory, then the last memory address in the range will be <code>1023</code>, as we know, <code>1MB</code> = <code>1024 bytes</code> and since the range starts from <code>0</code> and not <code>1</code>, then the last memory address in this case is <code>1023</code> and not <code>1024</code>. This range of memory addresses is known as <em>address space</em> and it can be a <em>physical address space</em> which is limited by the physical main memory or a <em>logical address space</em>. A well-known example of using logical address space that we will discuss in a latter chapter is <em>virtual memory</em> which provides a logical address space of size <code>4GB</code> in <code>32-bit</code> architecture even if the actual size of physical main memory is less than <code>4GB</code>. However, The address space starts from the memory address <code>0</code>, which is the index of the first cell (byte) of the memory, and it increases by <code>1</code>, so the memory address <code>1</code> is the index of the second cell of the memory, <code>2</code> is the index of third cell of memory and so on. Viewing the memory as an array of contiguous cells is also known as <em>flat memory model</em>.</p>
<div id="fig:memory_physical_view" class="fignos">
<div class="figure">
<img src="Figures/x86-ch/memory-physical-view.png" alt="Figure 1: The Physical View of the Memory. The Size of it is n Bytes." style="width:75.0%" />
<p class="caption"><span>Figure 1:</span> The Physical View of the Memory. The Size of it is <code>n</code> Bytes.</p>
</div>
</div>
<p>When we say <em>physical</em> we mean the actual hardware, that is, when the maximum capacity of the hardware of the main memory (RAM) is <code>1MB</code> then the physical address space of the machine is up to <code>1MB</code>. On the other hand, when we say <em>logical</em> that means it doesn't necessarily represents or obeys the way the actual hardware works on, instead it is a hypothetical way of something that doesn't exist in the real world (the hardware). To make the <em>logical</em> view of anything works, it should be mapped into the real <em>physical</em> view, that is, it should be somehow translated for the physical hardware to be understood, this mapping is handled by the software or sometimes special parts of the hardware.</p>
<p>Now, for the following discussion, let me remind you that the memory address is just a numerical value, it is just a number. When I discuss the memory address as a mere number I call it <em>memory address value</em> or <em>the value of memory address</em>, while the term <em>memory address</em> keeps its meaning, which is a unique identifier that refers to a specific location (cell) in the main memory.</p>
<p>The values of memory addresses are used by the processor all the time to be able to perform its job, and when it is executing some instructions that involve the main memory (e.g. reading a content from some memory location or dealing with program counter), the related values of memory addresses are stored temporarily on the registers of the processor, due to that, the length of a memory address value is bounded to the size of the processor's registers, so, in <code>32-bit</code> environments, where the size of the registers is usually <code>32-bit</code>, the length of the memory address value is <strong>always</strong> <code>32</code> bits, why am I stressing &quot;always&quot; here? Because even if less than <code>32</code> bits is enough to represent the memory address value, it will be represented in <code>32</code> bits though, for example, assume the memory address value <code>1</code>, in binary, the value <code>1</code> can be represented by only <code>1 bit</code> and no more, but in reality, when it is stored (and handled) by the <code>32-bit</code> processor, it will be stored as the following sequence of bits.</p>
<div class="sourceCode"><pre class="sourceCode c"><code class="sourceCode c"><span class="dv">00000000</span> <span class="dv">00000000</span> <span class="dv">00000000</span> <span class="dv">00000001</span></code></pre></div>
<p>As you can see, the value <code>1</code> has been represented in exactly <code>32</code> bits, appending zeros to the left doesn't change the value itself, it is similar to writing a number as <code>0000539</code> which is exactly <code>539</code>.</p>
<p>It has been mentioned earlier that the register size that stores the values of memory address in order to deal with memory contents affects the available size of main memory for the system. Take for example the instruction pointer register, if its size, say, <code>16</code> bits then the maximum available memory for code will be <code>64KB</code> (<code>64</code> KB = <code>65536</code> Bytes / <code>1024</code>) since it is the last reachable memory address by the processor for fetching an instruction. What if the size of the instruction pointer register is <code>32</code> bits, then the maximum available memory for code will be <code>4GB</code>. Why is that?</p>
<p>To answer this question let's work with decimal numbers first. If I tell you that you have five blanks, what is the largest decimal number you can represent in these five blanks? the answer is <code>99999d</code>. In the same manner, if you have <code>5</code> blanks, what is the largest binary number you can represent in these 5 blanks? it is <code>11111b</code> which is equivalent to <code>31d</code>, the same holds true for the registers that store the value of memory addresses, given the size of such register is <code>16</code> bits, then there is <code>16</code> blanks, and the largest binary number that can be represented in those <code>16</code> blanks is <code>11111111 11111111b</code> or in hexadecimal <code>FF FFh</code>, which is equivalent to <code>65535d</code>, that means the last byte a register of size <code>16</code> bits can refer to is the byte number <code>65535d</code> because it is the largest value this register can store and no more, which leads to the maximum size of main memory this register can handle, it is <code>65535 bytes</code> which is equivalent to <code>64KB</code> and the same applies on any other size than <code>16</code> bits.</p>
<h2 id="x86-segmentation"><span class="header-section-number">2.5</span> x86 Segmentation</h2>
<p>The aforementioned view of memory, that is, the <em>addressable array of bytes</em> can be considered as the <em>physical</em> view of the main memory which specifies the mechanism of accessing the data. On top of this physical view a <em>logical</em> view can be created and one example of logical views is <em>x86 segmentation</em>.</p>
<p>In x86 segmentation the main memory is viewed as separated parts called <em>segments</em> and each segment stores a bunch of related data. To access data inside a segment, each byte can be referred to by its own <em>offset</em>. The running program can be separated into three possible types of segments in x86, these types are: <em>code segment</em> which stores the code of the program under execution, <em>data segments</em> which store the data of the program and the <em>stack segment</em> which stores the data of program's stack. Segmentation is the default view of memory in x86 and it's unavoidable and the processor always run with the mind that the running program is divided into segments, however, most modern operating system choose to view the memory as the one described in flat memory model instead of viewing it as segmented areas, to be able to implement flat memory model in x86 which doesn't allow to disable segmentation, at least two segments (one for code and one for data) should be defined in the system, and the size of both segments should be same as physical memory's size and both of segments start from the first memory address <code>0</code> and ends in the last memory address (memory size - 1), that is, these both segments will overlap.</p>
<div id="fig:memory_segmented_view" class="fignos">
<div class="figure">
<img src="Figures/x86-ch/memory-segmented-view.png" alt="Figure 2: An Example of The Segmented View of the Memory" style="width:35.0%" />
<p class="caption"><span>Figure 2:</span> An Example of The Segmented View of the Memory</p>
</div>
</div>
<h3 id="segmentation-in-real-mode"><span class="header-section-number">2.5.1</span> Segmentation in Real Mode</h3>
<p>For the sake of clarity, let's discuss the details of segmentation under real mode first. We have said that logical views (of anything) should be mapped to the physical view either by software or hardware, in this case, the segmentation view is realized and mapped to the architecture of the physical main memory by the x86 processor itself, that is, by the hardware. So, we have a logical view, which is the concept of segmentation which divides a program into separated segments, and the actual physical main memory view which is supported by the real RAM hardware and sees the data as a big array of bytes. Therefore, we need some tools to implement (map) the logical view of segmentation on top the actual hardware.</p>
<p>For this purpose, special registers named <em>segment registers</em> are presented in x86, the size of each segment register is <code>16</code> bits and they are: <code>CS</code> which is used to define the code segment. <code>SS</code> which is used to define the stack segment. <code>DS</code>, <code>ES</code>, <code>FS</code> and <code>GS</code> which can be used to define data segments, that means each program can have up to four data segments. Each segment register stores the <em>starting memory address</em> of a segment and here you can start to observe the mapping between the logical and physical view. In real mode, the size of each segment is <code>64KB</code> and as we have said we can reach any byte inside a segment by using the <em>offset</em> of the required byte, you can see the resemblance between a memory address of the basic view of memory and an offset of the segmentation view of memory <a href="#fn10" class="footnoteRef" id="fnref10"><sup>10</sup></a>.</p>
<p>Let's take an example to make the matter clear, assume that we have a code of some program loaded into the memory and its starting physical memory address is <code>100d</code>, that is, the first instruction of this program is stored in this address and the next instructions are stored right after this memory address one after another. To reach the first byte of this code we use the offset <code>0</code>, so, the whole physical address of the first byte will be <code>100:0d</code>, as you can see, the part before the colon is the starting memory address of the code and the part after the colon is the offset that we would like to reach and read the byte inside it. In the same way, let's assume we would like to reach the offset <code>33</code>, which means the byte <code>34</code> inside the loaded code, then the physical address that we are trying to reach is actually <code>100:33d</code>. To make the processor handle this piece of code as the <em>current</em> code segment then its starting memory address should be loaded into the register <code>CS</code>, that is, setting the value <code>100d</code> to <code>CS</code>, so, we can say in other words that <code>CS</code> contains the starting memory address of currently executing code segment (for short: current code segment).</p>
<p>As we have said, the x86 processor always run with the mind that the segmentation is in use. So, let's say it is executing the following assembly instruction <code>jmp 150d</code> which jumps to the address <code>150d</code>. What really happens here is that the processor consider the value <code>150d</code> as an offset instead of a full memory address, so, what the instruction requests from the processor here is to jump to the offset <code>150</code> which is inside the current code segment, therefore, the processor is going to retrieve the value of the register <code>CS</code> to know what is the starting memory address of the currently active code segment and append the value <code>150</code> to it. Say, the value of <code>CS</code> is <code>100</code>, then the memory address that the processor is going to jump to is <code>100:150d</code>.</p>
<p>This is also applicable on the internal work of the processor, do you remember the register <code>IP</code> which is the instruction pointer? It actually stores the offset of the next instruction instead of the whole memory address of the instruction. Any call (or jump) to a code inside the same code segment of the caller is known as <em>near call (or jump)</em>, otherwise is it a <em>far call (or jump)</em>. Again, let's assume the current value of <code>CS</code> is <code>100d</code> and you want to call a label which is on the memory location <code>900:1d</code>, in this situation you are calling a code that reside in a different code segment, therefore, the processor is going to take the first part of the address which is<code>900d</code>, loads it to <code>CS</code> then loads the offset <code>1d</code> in <code>IP</code>. Because this call caused the change of <code>CS</code> value to another value, it is a far call.</p>
<p>The same is exactly applicable to the other two types of segments and of course, the instructions deal with different segment types based on their functionality, for example, you have seen that <code>jmp</code> and <code>call</code> deal the code segment in <code>CS</code>, that's because of their functionality which is related to the code. Another example is the instruction <code>lodsb</code> which deals with the data segment <code>DS</code>, the instruction <code>push</code> deals with the stack segment <code>SS</code> and so on.</p>
<h4 id="segmentation-used-in-the-bootloader"><span class="header-section-number">2.5.1.1</span> Segmentation Used in the Bootloader</h4>
<p>In the previous chapter, when we wrote the bootloader, we have dealt with the segments. Let's get back to the source code of the bootloader, you remember that the firmware loads the bootloader on the memory location <code>07C0h</code> and because of that we started our bootloader with the following lines.</p>
<div class="sourceCode"><pre class="sourceCode asm"><code class="sourceCode fasm">    <span class="bu">mov</span> <span class="kw">ax</span><span class="bn">, 07C0h</span>
    <span class="bu">mov</span> <span class="kw">ds</span>, <span class="kw">ax</span></code></pre></div>
<p>Here, we told the processor that the data segment of our program (the bootloader) starts in the memory address <code>07C0h</code> <a href="#fn11" class="footnoteRef" id="fnref11"><sup>11</sup></a>, so, if we refer to the memory to read or write <strong>data</strong>, the processor starts with the memory address <code>07C0h</code> which is stored in the data segment register <code>ds</code> and then it appends the offset that we are referring to, in other words, any reference to data by the code being executed will make the processor to use the value in data segment register as the beginning of the data segment and the offset of referred data as the rest of the address, after that, this physical memory address of the referred data will be used to perform the instruction. An example of instructions that deal with data in our bootloader is the line <code>mov si, title_string</code>.</p>
<p>Now assume that BIOS has set the value of <code>ds</code> to <code>0</code> (it can be any other value) and jumped to our bootloader, that means the data segment in the system now starts from the physical memory address <code>0</code> and ends at the physical memory address <code>65535</code> since the maximum size of a segment in real-mode is 64KB. Now let's take the label <code>title_string</code> as an example and let's assume that its offset in the binary file of our bootloader is <code>490</code>, when the processor starts to execute the line <code>mov si, title_string</code> <a href="#fn12" class="footnoteRef" id="fnref12"><sup>12</sup></a> it will, somehow, figures that the offset of <code>title_string</code> is <code>490</code> and based on the way that x86 handles memory accesses the processor is going to think that we are referring to the physical memory address <code>490</code> since the value of <code>ds</code> is <code>0</code>, but in reality, the correct physical memory address of <code>title_string</code> is the offset <code>490</code> <strong>inside</strong> the memory address <code>07C0h</code> since our bootloader is loaded into this address and not the physical memory address <code>0</code>, so, to be able to reach to the correct addresses of the data that we have defined in our bootloader and that are loaded with the bootloader starting from the memory address <code>07C0h</code> we need to tell the processor that our data segment starts from <code>07C0h</code> and with any reference to data, it should calculate the offset of that data starting from this physical address, and that exactly what these two lines do, in other words, change the current data segment to another one which starts from the first place of our bootloader.</p>
<p>The second use of the segments in the bootloader is when we tried to load the kernel from the disk by using the BIOS service <code>13h:02h</code> in the following code.</p>
<div class="sourceCode"><pre class="sourceCode asm"><code class="sourceCode fasm">    <span class="bu">mov</span> <span class="kw">ax</span><span class="bn">, 0900h</span>
    <span class="bu">mov</span> <span class="kw">es</span>, <span class="kw">ax</span>
    
    <span class="bu">mov</span><span class="bn"> ah, 02h</span>
    <span class="bu">mov</span> <span class="kw">al</span><span class="bn">, 01h</span>
    <span class="bu">mov</span><span class="bn"> ch, 0h</span>
    <span class="bu">mov</span> <span class="kw">cl</span><span class="bn">, 02h</span>
    <span class="bu">mov</span><span class="bn"> dh, 0h</span>
    <span class="bu">mov</span> <span class="kw">dl</span><span class="bn">, 80h</span>
    <span class="bu">mov</span> <span class="kw">bx</span><span class="bn">, 0h</span>
    <span class="bu">int</span><span class="bn"> 13h</span></code></pre></div>
<p>You can see here, we have used the other data segment <code>ES</code> to define a new data segment that starts from the memory address <code>0900h</code>, we did that because the BIOS service <code>13h:02h</code> loads the required content (in our case the kernel) to the memory address <code>ES:BX</code>, for that, we have defined the new data segment and set the value of <code>bx</code> to <code>0h</code>. That means the code of the kernel will be loaded on <code>0900:0000h</code> and because of that, after loading the kernel successfully we have performed a far jump.</p>
<div class="sourceCode"><pre class="sourceCode asm"><code class="sourceCode fasm"><span class="bu">jmp</span> 0900h:<span class="dv">0000</span></code></pre></div>
<p>Once this instruction is executed, the value of <code>CS</code> will be changed from the value <code>07C0h</code>, where the bootloader resides, to the value <code>0900h</code> where the kernel resides and the value of <code>IP</code> register will be <code>0000</code> then the execution of the kernel is going to start.</p>
<h3 id="segmentation-in-protected-mode"><span class="header-section-number">2.5.2</span> Segmentation in Protected Mode</h3>
<p>The fundamentals of segmentation in protected mode is exactly same as the ones explained in real mode, but it has been extended to provide more features such as <em>memory protection</em>. In protected mode, a table named <em>global descriptor table</em> (<code>GDT</code>) is presented, this table is stored in the main memory and its starting memory address is stored in the special purpose register <code>GDTR</code> as a reference, each entry in this table called a <em>segment descriptor</em> which has the size <code>8</code> bytes and they can be referred to by an index number called <em>segment selector</em> <a href="#fn13" class="footnoteRef" id="fnref13"><sup>13</sup></a> which is the offset of the entry inside <code>GDT</code> table, For example, the offset of the first entry in <code>GDT</code> is <code>0</code>, and adding this offset with the value of <code>GDTR</code> gives us the memory address of that entry, however, the first entry of <code>GDT</code> should not be used by the operating system.</p>
<p>An entry of <code>GDT</code> (a segment descriptor), defines a segment (of any type) and has the information that is required by the processor to deal with that segment. The starting memory address of the segment is stored in its descriptor <a href="#fn14" class="footnoteRef" id="fnref14"><sup>14</sup></a>, also, the size (or limit) of the segment. The segment selector of the currently active segment should be stored in the corresponding segment register.</p>
<p>To clarify the matter, consider the following example. Let's assume we are currently running two programs and their code are loaded into the main memory and we would like to separate these two pieces of code into a couple of code segments. The memory area <code>A</code> contains code of the first program and starts from the memory address <code>800</code> while the memory area <code>B</code> contains the code of the second program<code>B</code> and starts in the memory address <code>900</code>. Assume that the starting memory address of <code>GDT</code> is <code>500</code> which is already loaded in <code>GDTR</code>.</p>
<p>To make the processor consider <code>A</code> and <code>B</code> as code segments we should define a segment descriptor for each one of them. We already know that the size of a segment descriptor is <code>8</code> bytes, so, if we define a segment descriptor for the segment <code>A</code> as entry <code>1</code> (remember that the entries on <code>GDT</code> starts from zero) then its offset (segment selector) in <code>GDT</code> will be <code>8</code> (<code>1 * 8</code>), the segment descriptor of <code>A</code> should contain the starting address of <code>A</code> which is <code>800</code>, and we will define the segment descriptor of <code>B</code> as entry <code>2</code> which means its offset (segment selector) will be <code>16</code> (<code>2 * 8</code>).</p>
<p>Let's assume now that we want the processor to execute the code of segment <code>A</code>, we already know that the processor consults the register <code>CS</code> to decide which code segment is currently active and should be executed next, for that, the <strong>segment selector</strong> of code segment <code>A</code> should be loaded in <code>CS</code>, so the processor can start executing it. In real mode, the value of <code>CS</code> and all other segment registers was a memory address, on the other hand, in protected mode, the value of <code>CS</code> and all other segment registers is a segment selector.</p>
<p>In our situation, the processor takes the segment selector of <code>A</code> from <code>CS</code> which is <code>8</code> and starting from the memory address which is stored in <code>GDTR</code> it walks <code>8</code> bytes, so, if <code>GDTR = 500</code>, the processor will find the segment descriptor of <code>A</code> in the memory address <code>508</code>. The starting address of <code>A</code> will be found in its segment descriptor and the processor can use it with the value of register <code>EIP</code> to execute <code>A</code>'s code. Let's assume a far jump is occurred from <code>A</code> to <code>B</code>, then the value of <code>CS</code> will be changed to the segment selector of <code>B</code> which is <code>16</code>.</p>
<h4 id="the-structure-of-segment-descriptor"><span class="header-section-number">2.5.2.1</span> The Structure of Segment Descriptor</h4>
<p>A segment descriptor is an <code>8</code> bytes entry of global descriptor table which stores multiple <em>fields</em> and <em>flags</em> that describe the properties of a specific segment in the memory. With each memory reference to any segment, the processor is going to consult the descriptor that describes the segment in question to obtain basic information like starting memory address of this segment.</p>
<p>Beside the basic information, a segment descriptor stores information the helps in memory protection, due to that, segmentation in x86 protected-mode is considered as a way for memory protection and not a mere logical view of the memory, so each memory reference is being monitored by the processor.</p>
<p>By using those properties that are related to memory protection, the processor will be able to protect the different segments on the system from each other and not letting some less privileged to call a code or manipulate data which belong to more privileged area of the system, a concrete example of that is when a userspace software (e.g. Web Browser) tries to modify an internal data structure in the kernel.</p>
<p>In the following subsections, each field and flag of segment descriptor will be explained, but before getting started we need to note that in here and in Intel's official x86 manual the term <em>field</em> is used when the size of the value that should be stored in the descriptor is <strong>more than</strong> <code>1</code> bit, for example the segment's starting memory address is stored in <code>4</code> bytes, then the place where this address is stored in the descriptor is called a field, otherwise when the term <em>flag</em> is used that means the size of the value is <code>1</code> bit.</p>
<h5 id="segments-base-address-and-limit"><span class="header-section-number">2.5.2.1.1</span> Segment's Base Address and Limit</h5>
<p>The most important information about a segment is its starting memory address, which is called the <em>base address</em> of a segment. In real mode, the base address was stored in the corresponding segment register directly, but in protected mode, where we have more information about a segment than mere base address, this information is stored in the descriptor of the segment <a href="#fn15" class="footnoteRef" id="fnref15"><sup>15</sup></a>.</p>
<p>When the currently running code refers to a memory address to read from it, write to it (in the case of data segments) or call it (in the case of code segments) it is actually referring to a specific segment in the system <a href="#fn16" class="footnoteRef" id="fnref16"><sup>16</sup></a>. For the simplicity of next discussions, we call this memory address, which is referenced by the currently running code, the <em>generated memory address</em> because, you know, it is generated by the code.</p>
<p>Any generated memory address in x86 architecture is not an actual physical memory address <a href="#fn17" class="footnoteRef" id="fnref17"><sup>17</sup></a>, that means, if you hypothetically get a generated memory address and try to get the content of its physical memory location, the obtained data will not be same as the data which is required by the code. Instead, a generated memory address is named by Intel's official manual a <em>logical memory address</em> because, you know, it is not real memory address, <strong>it is</strong> logical. Every logical memory address refers to some byte in a specific segment in the system, and to be able to obtain the data from the actual physical memory, this logical memory address should be <em>translated</em> to a <em>physical memory address</em> <a href="#fn18" class="footnoteRef" id="fnref18"><sup>18</sup></a>.</p>
<p>The logical memory address in x86 may pass <strong>two</strong> translation processes instead of one in order to obtain the physical memory address. The first address translation is performed on a logical memory address to obtain a <em>linear memory address</em> which is another not real and not physical memory address which is there in x86 architecture because of paging feature. If paging <a href="#fn19" class="footnoteRef" id="fnref19"><sup>19</sup></a> is enabled in the system, a second translation process takes place on this linear memory address to obtain the real physical memory address. If paging is disabled, the linear memory address which is generated by the first translation process is same as the physical memory address. We can say that the first translation is there in x86 due to the segmentation view of memory, while the second translation is there in x86 due to the paging view of memory.</p>
<div id="fig:logical_memory_address_translation" class="fignos">
<div class="figure">
<img src="Figures/x86-ch/logical-memory-address-translation.png" alt="Figure 3: Shows How a Logical Memory Address is Translated to a Linear Memory Address (Which Represents a Physical Address when Paging is Disabled)." style="width:90.0%" />
<p class="caption"><span>Figure 3:</span> Shows How a Logical Memory Address is Translated to a Linear Memory Address (Which Represents a Physical Address when Paging is Disabled).</p>
</div>
</div>
<p>For now, our focus will be on the translation from a logical memory address to a linear memory address which is same as the physical memory address since paging feature is disabled by default in x86. Each logical memory address consists of two parts, a <code>16</code> bits segment selector and a <code>32</code> bits offset. When the currently running code generates a logical memory address (for instance, to read some data from memory) the processor needs to perform the translation process to obtain the physical memory address as the following. First, it reads the value of the register <code>GDTR</code> which contains the starting physical memory address of <code>GDT</code>, then it uses the <code>16-bit</code> segment selector in the generated logical address to locate the descriptor of the segment that the code would like to read the data from, inside segment's descriptor, the physical base address (the starting physical address) of the requested segment can be found, the processor obtains this base address and adds the <code>32-bit</code> offset from the logical memory address to the base address to obtain the last result, which is the linear memory address.</p>
<p>During this operation, the processor uses the other information in the segment descriptor to enforce the policies of memory protection. One of these policies is defined by the <em>limit</em> of a segment which specifies its size, if the generated code refers to an offset which exceeds the limit of the segment, the processor should stop this operation. For example, assume hypothetically that the running code has the privilege to read data from data segment <code>A</code> and in the physical memory another data segment <code>B</code> is defined right after the limit of <code>A</code>, which means if we can exceed the limit of <code>A</code> we will able to access the data inside <code>B</code> which is a critical data segment that stores kernel's internal data structures and we don't want any code to read from it or write to it in case this code is not privileged to do so. This can be achieved by specifying the limit of <code>A</code> correctly, and when the unprivileged code tries maliciously to read from <code>B</code> by generating a logical memory address that has an offset which exceeds the limit of <code>A</code> the processor prevents the operation and protects the content of segment <code>B</code>.</p>
<p>The limit, or in other words, the size of a given segment is stored in the <code>20</code> bits <em>segment limit field</em> of that segment descriptor and how the processor interprets the value of segment limit field depends on the <em>granularity flag</em> (G flag) which is also stored in the segment's descriptor, when the value of this flag is <code>0</code> then the value of the limit field is interpreted as bytes, let's assume that the limit of a given segment is <code>10</code> and the value of granularity flag is <code>0</code>, that means the size of this segment is <code>10</code> <strong>bytes</strong>. On the other hand, when the value of granularity flag is <code>1</code>, the value of segment limit field will be interpreted as of <code>4KB</code> units, for example, assume in this case that the value of limit field is also <code>10</code> but G flag = <code>1</code>, that means the size of the segment will be <code>10</code> of <code>4KB</code> units, that is, <code>10 * 4KB</code> which gives us <code>40KB</code> which equals <code>40960</code> bytes.</p>
<p>Because the size of segment limit field is <code>20</code> bits, that means the maximum numeric value it can represent is <code>2^20 = 1,048,576</code>, which means if G flag equals <code>0</code> then the maximum size of a specific segment can be <code>1,048,576</code> <strong>bytes</strong> which equals <code>1MB</code>, and if G flag equals <code>1</code> then the maximum size of a specific segment can be <code>1,048,576</code> of <code>4KB</code> units which equals <code>4</code> <strong>GB</strong>.</p>
<p>Getting back to the structure of descriptor, the bytes <code>2</code>, <code>3</code> and <code>4</code> of the descriptor store the <em>least significant bytes</em> of segment's base address and the byte <code>7</code> of the descriptor stores the <em>most significant byte</em> of the base address, the total is <code>32</code> bits for the base address. The bytes <code>0</code> and <code>1</code> of the descriptor store the <em>least significant bytes</em> of segment's limit and byte <code>6</code> stores the <em>most significant byte</em> of the limit. The granularity flag is stored in the most significant <strong>bit</strong> of the the byte <code>6</code> of the descriptor.</p>
<p>Before finishing this subsection, we need to define the meaning of <em>least significant</em> and <em>most significant</em> byte or bit. Take for example the following binary sequence which may represent anything, from a memory address value to a <code>UTF-32</code> character.</p>
<p><strong>0</strong>111 0101 0000 0000 0000 0000 0100 110<em>1</em></p>
<p>You can see the first bit from left is on bold format and its value is <code>0</code>, based on its position in the sequence we call this bit the <em>most significant bit</em> or <em>high-order bit</em>, while the last bit on the right which is in italic format and its value is <code>1</code> is known as <em>least significant bit</em> or <em>low-order bit</em>. The same terms can be used on byte level, given the same sequence with different formatting.</p>
<p><strong>0111 0101</strong> 0000 0000 0000 0000 <em>0100 1101</em></p>
<p>The first byte (<code>8</code> bits) on the left which is in bold format and its value is <code>0111 0101</code> is known as <em>most significant byte</em> or <em>high-order byte</em> while the last byte on the right which is on italic format and its value is <code>0100 1101</code> is known as <em>least significant byte</em> or <em>low-order byte</em>.</p>
<p>Now, imagine that this binary sequence is the base address of a segment, then the least significant <code>3</code> bytes of it will be stored in bytes <code>2</code>, <code>3</code> and <code>4</code> of the descriptor, that is, the following binary sequence.</p>
<div class="sourceCode"><pre class="sourceCode asm"><code class="sourceCode fasm"><span class="dv">0000</span> <span class="dv">0000</span> <span class="dv">0000</span> <span class="dv">0000</span> <span class="dv">0100</span> <span class="dv">1101</span></code></pre></div>
<p>While the most significant byte of the binary sequence will be stored in the <code>7th</code> byte of the descriptor, that is, the following binary sequence.</p>
<div class="sourceCode"><pre class="sourceCode asm"><code class="sourceCode fasm"><span class="dv">0111</span> <span class="dv">0101</span></code></pre></div>
<h5 id="segments-type"><span class="header-section-number">2.5.2.1.2</span> Segment's Type</h5>
<p>Given any binary sequence, it doesn't have any meaning until some context is added. For example, what does the binary sequence <code>1100 1111 0000 1010</code> represents? It could represent anything, a number, characters, pixels on an image or even all of them based on how its user interprets it. When an agent (e.g. a bunch of code in running software or the processor) works with a binary sequence, it should know what does this binary sequence represent to be able to perform useful tasks. In the same manner, when a segment is defined, the processor (the agent) should be told how to interpret the content inside this segment, that is, the type of the segment should be known by the processor.</p>
<p>Till this point, you probably noticed that there is at least two types of segments, code segment and data segment. The content of the former should be machine code that can be executed by the processor to perform some tasks, while the content of the latter should be data (e.g. values of constants) that can be used by a running code. These two types of segments (code and data) belong to the category of <em>application segments</em>, there is another category of segment types which is the category of <em>system segments</em> and it has many different segment types belong to it.</p>
<p>Whether a specific segment is an application or system segment, this should be mentioned in the descriptor of the segment in a flag called <em>S flag</em> or <em>descriptor type flag</em> which is the fifth <strong>bit</strong> in <strong>byte</strong> number <code>5</code> of the segment descriptor. When the value of S flag is <code>0</code>, then the segment is considered as a system segment, while it is considered as an application segment when the value of S flag is <code>1</code>. Our current focus is on the latter case.</p>
<p>As we have mentioned before, an application segment can be either code or data segment. Let's assume some application segment has been referenced by a currently running code, the processor is going to consult the descriptor of this segment, and by reading the value of S flag (which should be <code>1</code>) it will know that the segment in question is an application segment, but which of the two types? Is it a code segment or data segment? To answer this question for the processor, this information should be stored in a field called <em>type field</em> in the segment's descriptor.</p>
<p>Type field in segment descriptor is the first <code>4</code> bits (nibble) of the fifth byte and the most significant bit specifies if the application segment is a code segment (when the value of the bit is <code>1</code>) or a data segment (when the value of the bit is <code>0</code>). Doesn't matter if the segment is a code or data segment, in the both cases the least significant bit of type field indicates if the segment is <em>accessed</em> or not, when the value of this flag is <code>1</code>, that means the segment has been written to or read from (AKA: accessed), but if the value of this flag is <code>0</code>, that means the segment has not been accessed. The value of this flag is manipulated by the processor in one situation only, and that's happen when the selector of the segment in question is loaded into a segment register. In any other situation, it is up to the operating system to decide the value of accessed flag. According to Intel's manual, this flag can be used for virtual memory management and for debugging.</p>
<h6 id="code-segment-flags"><span class="header-section-number">2.5.2.1.2.1</span> Code Segment Flags</h6>
<p>When the segment is a code segment, the second most significant bit (tenth bit) is called <em>conforming flag</em> (also called <code>C</code> flag) while the third most significant bit (ninth bit) called <em>read-enabled flag</em> (also called <code>R</code> flag.). Let's start our discussion with the simplest among those two flags which is the read-enabled flag. The value of this flag indicates how the code inside the segment in question can be used, when the value of read-enabled flag is <code>1</code> <a href="#fn20" class="footnoteRef" id="fnref20"><sup>20</sup></a>, that means the content of the code segment can be executed <strong>and</strong> read from, but when the value of this flag is <code>0</code> <a href="#fn21" class="footnoteRef" id="fnref21"><sup>21</sup></a> that means the content of the code segment can be <strong>only</strong> executed and cannot read from. The former option can be useful when the code contains data inside it (e.g. constants) and we would like to provide the ability of reading this data. When read is enabled for the segment in question, the selector of this segment can also be loaded into one of data segment registers <a href="#fn22" class="footnoteRef" id="fnref22"><sup>22</sup></a>.</p>
<p>The conforming flag is related to the privilege levels that we had an overview about them previously in this chapter. When a segment is conforming, in other words, the value of conforming flag is <code>1</code>, that means a code which runs in a less-privileged level can call this segment which runs in a higher privileged level while keeping the current privilege level of the environment same as the one of the caller instead of the callee.</p>
<p>For example, let's assume for some reason a kernel's designer decided to provide simple arithmetic operations (such as addition and subtraction) for user applications from the kernel code, that is, there is no other way to perform these operations in that system but this code which is provided by the kernel. As we know, kernel's code should run in privilege level <code>0</code> which is the most-privileged level, and let's assume a user application which runs in privilege level <code>3</code>, a less-privileged level, needs to perform an addition operation, in this case a kernel code, which should be protected by default from being called by less-privileged code, should be called to perform the task, this can only realized if the code of addition operation is provided as a conforming segment, otherwise the processor is going to stop this action where a less-privileged code calls a more-privileged code.</p>
<p>Also you should note that the code of addition operation is going to run in privilege level <code>3</code> although it is a part of the kernel which runs in privilege level <code>0</code> and that's because of the original caller which runs in the privilege level <code>3</code>. Furthermore, although conforming segment can be called by a less-privilege code (e.g. user application calls the kernel), the opposite cannot be done (e.g. the kernel calls a user application's code) and the processor is going to stop the operation.</p>
<h6 id="data-segment-flags"><span class="header-section-number">2.5.2.1.2.2</span> Data Segment Flags</h6>
<p>When the segment is data segment, the second most significant bit (tenth bit) is called expansion-direction flag (also called <code>E</code> flag) while the third most significant bit (ninth bit) is called write-enabled flag (also called <code>W</code> flag). The latter one gives us the ability to make some data segment a read-only when its value is <code>0</code>, or we can make a data segment both <strong>writable</strong> and readable by setting the value of write-enabled flag to <code>1</code>.</p>
<p>While the expansion-direction flag and its need will be examined in details when we discuss x86 run-time stack in this chapter, what we need to know right now is that when the value of this flag is <code>0</code>, the data segment is going to expand <strong>up</strong> (in Intel's terms), but when the value of this flag is <code>1</code>, the data segment is going to expand <strong>down</strong> (in Intel's terms).</p>
<p>A last note about data segments is that all of them are <strong>non-conforming</strong>, that is, a less-privileged code cannot access a data segment in a more-privileged level. Furthermore, all data segments can be accessed by a more-privileged code.</p>
<h5 id="segments-privilege-level"><span class="header-section-number">2.5.2.1.3</span> Segment's Privilege Level</h5>
<p>In our previous discussions, we have stated that a specific segment should belong to a privilege level and based on this privilege level the processor decides the protection properties of the segment in question, for example, whether that segment is a kernel-mode or user-mode segment and which privilege level a running code should belong to in order to be able to reach to this segment</p>
<p>A field called <em>descriptor privilege level</em> (DPL) in segment descriptor is where the operating system should set the privilege level of a given segment. The possible values of this field, as we know, are <code>0</code>, <code>1</code>, <code>2</code> and <code>3</code>, we have already discussed the meanings of these values previously in this chapter. Descriptor privilege level field occupies the second and third most significant bits of byte <code>5</code> in a descriptor.</p>
<h5 id="segments-present"><span class="header-section-number">2.5.2.1.4</span> Segment's Present</h5>
<p>One of common operations that is performed in a running system is loading data from secondary storage (e.g. hard disk) into the memory and one example of that is loading a program code into the memory when the user of the system request to run an instance of a program, so, creating a new segment descriptor (hence, creating new segment in the memory) for this data may precede the completion of loading the data into the main memory, therefore, there could be some segment descriptors in the system that points to memory locations that don't contain the real data yet.</p>
<p>In this case, we should tell the processor that the data in the memory location that a specific descriptor points to is not the real data, and the real segment is not presented in the memory yet, this helps the processor to generate error when some code tries to access <a href="#fn23" class="footnoteRef" id="fnref23"><sup>23</sup></a> the segment's data. To tell the processor whether a segment is presented in the memory or not, <em>segment-present flag</em> (P flag) can be used, when its value is <code>1</code> that means the segment is present in memory, while the value <code>0</code> means the segment is not present in memory, this flag is the most significant bit of byte <code>5</code> of a descriptor.</p>
<h5 id="other-flags"><span class="header-section-number">2.5.2.1.5</span> Other Flags</h5>
<p>We have covered all segment's descriptor fields and flags but three flags. The name of the first one changes depending on the type of the segment and it occupies the second most significant bit in the byte <code>6</code>. When the segment in question is a code segment, this flag is called <em>default operation size</em> (D flag). When the processor executes the instructions it uses this flag to decide the length of the operands, depending on the currently executing instruction, if the value of D flag is <code>1</code> the processor is going to assume the operand has the size of <code>32</code> bits if it is a memory address, and <code>32</code> bits or <code>8</code> bits if it is not a memory address. If the value of D flag is <code>0</code> the processor is going to assume the operand has the size of <code>16</code> bits if it is a memory address, and <code>16</code> bits or <code>8</code> bits operand if it is not a memory address.</p>
<p>When the segment in question is a stack segment <a href="#fn24" class="footnoteRef" id="fnref24"><sup>24</sup></a>, the same flag is called <em>default stack pointer size</em> (B flag), and it decides the size of the memory address (as a value) which points to the stack, this memory address is known as <em>stack pointer</em> and it is being used implicitly by stack instructions such as <code>push</code> and <code>pop</code>. When the value of B flag is <code>1</code>, then the size of stack pointer will be <code>32</code> bits and its value will be stored in the register <code>ESP</code> (rather then <code>SP</code>), When the value of B flag is <code>0</code>, then the size of stack pointer will be <code>16</code> bits and its value will be stored in the register <code>SP</code> (rather then <code>ESP</code>).</p>
<p>When the segment in question is a data segment that grows upward, this flag is called <em>upper bound flag</em> (B flag), when its value is <code>1</code> the maximum possible size of the segment will be <code>4GB</code>, otherwise, the maximum possible size of the segment will be <code>64KB</code>. Anyway, the value of this flag (D/B flag) <strong>should</strong> be <code>1</code> for <code>32-bit</code> code and data segments (stack segments are included of course) and it should be <code>0</code> for <code>16-bit</code> code and data segments.</p>
<p>The second flag is known as <em>64-bit code segment flag</em> (L flag) which is the third most significant bit in the byte <code>6</code> and from its name we can tell that this flag is related to code segments. If the value of this flag is <code>1</code> that means the code inside the segment in question is a <code>64-bit</code> code while the value <code>0</code> means otherwise <a href="#fn25" class="footnoteRef" id="fnref25"><sup>25</sup></a>. When we set the value of L flag to <code>1</code> the value of D/B flag should be <code>0</code>.</p>
<p>The final flag is the fourth most significant bit in the byte <code>6</code>, the value of this flag has no meaning for the processor, hence, it will not use it to make any decisions upon the segment in the question as we have seen on all other flags and fields. On the other hand, this flag is available for the operating system to use it in whatever way it needs, or just ignores it by set it to any of possible values ,<code>0</code> or <code>1</code> since it is one bit.</p>
<h4 id="the-special-register-gdtr"><span class="header-section-number">2.5.2.2</span> The Special Register <code>GDTR</code></h4>
<p>The special register <code>GDTR</code> stores the base physical address <a href="#fn26" class="footnoteRef" id="fnref26"><sup>26</sup></a> of the global descriptor table, that is, the starting point of <code>GDT</code> table. Also, the same register stores the limit (or size) of the table.</p>
<div id="fig:16062021_0" class="fignos">
<div class="figure">
<img src="Figures/x86-ch/Fig16062021_0.png" alt="Figure 4: GDTR Structure" style="width:50.0%" />
<p class="caption"><span>Figure 4:</span> GDTR Structure</p>
</div>
</div>
<p>To load a value into the register <code>GDTR</code> the x86 instruction <code>lgdt</code>, which stands for <em>load</em> global descriptor table, should be used. This instruction takes one operand which is the whole value that should be loaded into <code>GDTR</code>, the structure of this value should be similar to the structure of <code>GDTR</code> itself which is shown in figure <a href="#fig:16062021_0">4</a>. The figure shows that the total size of <code>GDTR</code> is <code>48</code> bits divided into two parts. The first part starts from bit <code>0</code> (the least significant bit) to bit <code>15</code>, this part contains the limit of <code>GDT</code> table that we would like to load. The size of this part of <code>GDTR</code> register is <code>16</code> bits which can represent the value <code>65,536</code> at maximum, that means the maximum size of <code>GDT</code> table can be <code>64KB = 65,536 Bytes / 1024</code>, and as we know, the size of each of descriptor is <code>8</code> bytes, that means the <code>GDT</code> table can hold <code>8,192</code> descriptors at most. The second part of <code>GDTR</code> starts from bit <code>16</code> to bit <code>47</code> (the most significant bit) and stores the base memory address of <code>GDT</code> table that we would like to load.</p>
<h4 id="local-descriptor-table"><span class="header-section-number">2.5.2.3</span> Local Descriptor Table</h4>
<p>The global descriptor table is a system-wide table, in other words, it is available for every process of the system. In addition to <code>GDT</code>, x86 provides us with ability to define <em>local descriptor tables</em> (<code>LDT</code>) in protected-mode which have the same functionality and structure of <code>GDT</code>.</p>
<p>In contrary to <code>GDT</code> table, multiple <code>LDT</code> can be defined in the system, and each one of them can be private to a specific process that is currently running on the system, also, multiple running processes can share a single <code>LDT</code> that is considered private for them by the kernel and no other processes can reach this given <code>LDT</code>. Anyway, how to use <code>LDT</code> depends on how the kernel is designed, and while <code>GDT</code> is required in x86 architecture by default, <code>LDT</code> on the other hand is optional and the designer of the kernel is the one who is responsible to decide whether to use <code>LDT</code> or not.</p>
<p>Let's assume that we need to create a new <code>LDT</code> table for process <code>A</code> which is currently running on the system, this <code>LDT</code> table is already filled with the descriptors that describe the segments which belong to process <code>A</code>. The structure of the descriptors in <code>LDT</code> is exactly same as the one that we already described in this chapter. To tell the processor that a given region of a memory is an <code>LDT</code> table, a new segment descriptor should be created in <code>GDT</code>.</p>
<p>In our previous discussion of <code>S</code> flag we mentioned that this flag tells the processor whether a defined segment is an application segment (S flag = <code>1</code>) or a system segment (S flag = <code>0</code>), the segment of the memory that contains an <code>LDT</code> table is considered as a system segment, that is, the value of <code>S</code> flag in the descriptor that describes an <code>LDT</code> table should be <code>0</code> and because there are other types of system segments than <code>LDT</code> then we should tell the processor this system segment is an <code>LDT</code> table, to do that we should use the type field of the descriptor that we already mentioned, the value of this field should be <code>0010b</code> (<code>2d</code>) for descriptors that describe an <code>LDT</code> table, how the processor can tell which table should currently used for a given segment <code>GDT</code> or <code>LDT</code> will be discussed in the next subsection.</p>
<p>The x86 instruction <code>lldt</code> is used to load the <code>LDT</code> table that we would like to use now into a special register named <code>LDTR</code> which is a <code>16-bit</code> register that should contain the segment selector <a href="#fn27" class="footnoteRef" id="fnref27"><sup>27</sup></a> in <code>GDT</code> of the <code>LDT</code> table that we would like to use, in other words, the index of segment descriptor which describe the <code>LDT</code> table and which reside in <code>GDT</code> as an entry should be loaded into <code>LDTR</code> register.</p>
<h4 id="segment-selector"><span class="header-section-number">2.5.2.4</span> Segment Selector</h4>
<p>As we know, the index (offset) of the descriptor that the currently running code needs to use, whether this descriptor defines and code or data segment, should be loaded into one of segment registers, but how the can the processor tell if this index which is loaded into a segment register is an index in the <code>GDT</code> or <code>LDT</code>?</p>
<div id="fig:17062021_0" class="fignos">
<div class="figure">
<img src="Figures/x86-ch/Fig17062021_0.png" alt="Figure 5: Segment Selector Structure" style="width:50.0%" />
<p class="caption"><span>Figure 5:</span> Segment Selector Structure</p>
</div>
</div>
<p>When we discussed segment selectors previously in this chapter we have said that our definition of this concept is a <strong>relaxed</strong> definition, that is, a simplified one that omits some details. In reality, the index of a segment descriptor is just one part of a segment selector, figure <a href="#fig:17062021_0">5</a> shows the structure of a segment selector, which is same as the structure of all segment registers <code>CS</code>, <code>SS</code>, <code>DS</code>, <code>ES</code>, <code>FS</code> and <code>GS</code>. We can see from the figure that the size of a segment selector is <code>16</code> bits and starting from the least significant bit (bit <code>0</code>), the first two bits <code>0</code> and <code>1</code> are occupied by field known as <em>requester privilege level</em> (<code>RPL</code>). Bit <code>2</code> is occupied by a flag named <em>table indicator</em> (<code>TI</code>), and finally, the index of a segment descriptor occupies the field from bit <code>3</code> until bit <code>15</code> of the segment selector. The index field (descriptor offset) is already well-explained in this chapter, so we don't need to repeat its details.</p>
<p>The table indicator flag (<code>TI</code>) is the one which is used by the processor to tell if the index in the segment selector is an index in <code>GDT</code>, when the value of <code>TI</code> is <code>0</code>, or <code>LDT</code>, when the value is <code>1</code>. When the case is the latter, the processor consults the register <code>LDTR</code> to know the index of the descriptor that defines the current <code>LDT</code> in the <code>GDT</code> and by using this index, the descriptor of <code>LDT</code> is read from <code>GDT</code> by the processor to fetch the base memory address of the current <code>LDT</code>, after that, the index in the segment selector register can be used to get the required segment descriptor from the current <code>LDT</code> by using the latter base memory address that just been fetched, of course these values are cached by the processor for quick future access.</p>
<p>In our previous discussions of privilege levels we have discussed two values, current privilege level (<code>CPL</code>) which is the privilege level of the currently executing code and descriptor privilege level (<code>DPL</code>) which is the privilege level of a given segment, the third value which contributes to the privilege level checks in x86 is <em>requester privilege level</em> (<code>RPL</code>) which is stored in the segment selector, necessarily, <code>RPL</code> has four possible values <code>0</code>, <code>1</code>, <code>2</code> and <code>3</code>.</p>
<p>To understand the role of <code>RPL</code> let's assume that a process <code>X</code> is running in a user-mode, that is, in privilege level <code>3</code>, this process is a malicious process that aims to gain an access to some important kernel's data structure, at some point of time the process <code>X</code> calls a code in the kernel and passes the segment selector of the more-privileged data segment to it as a parameter, the kernel code runs in the most privileged level and can access all privileged data segment by simply loading the required data segment selector to the corresponding segment register, in this case the <code>RPL</code> is set to <code>0</code> maliciously by process <code>X</code>, since the kernel runs on the privilege level <code>0</code> and <code>RPL</code> is <code>0</code>, the required segment selector by process <code>X</code> will be loaded and the malicious process <code>X</code> will be able to gain access to the data segment that has the sensitive data.</p>
<p>To solve this problem, <code>RPL</code> should be set to the <strong>requester</strong> privilege level by the kernel to load the required data segment, in our example, the requester (the caller) is the process <code>X</code> and its privilege level is <code>3</code> and the current privilege level is <code>0</code> since the kernel is running, but because the caller has a less-privileged level the kernel should set the <code>RPL</code> of the required data segment selector to <code>3</code> instead of <code>0</code>, this tells the processor that while the currently running code in a privilege level <code>0</code> the code that called it was running in privilege level <code>3</code>, so, any attempt to reach a segment which its selectors <code>RPL</code> is larger than <code>CPL</code> should be denied, in other words, the kernel should not reach privileged segments in behalf of process <code>X</code>. The x86 instruction <code>arpl</code> can be used by the kernel's code to change the <code>RPL</code> of the segment selector that has been requested by less-privileged code to access to the privilege level of the caller, as in the example of process <code>X</code>.</p>
<h2 id="x86-run-time-stack"><span class="header-section-number">2.6</span> x86 Run-time Stack</h2>
<p>A user application starts its life as a file stored in user's hard disk, at this stage it does nothing, it is just a bunch of binary numbers that represent the machine code of this application, when the user decides to use this application and opens it, the operating system loads this application into the memory and in this stage this user application becomes a process, we mentioned before that the term &quot;process&quot; is used in operating systems literature to describe a running program, another well-known term is <em>task</em> which is used by Linux kernel and has the same meaning.</p>
<p>Typically, the memory of a process is divided into multiple regions and each one of them stores a different kind of application's data, one of those regions stores the machine code of the application in the memory, there are also two important regions of process' memory, the first one is known as <em>run-time heap</em> (or just <strong>heap</strong> for short) which provides an area for dynamically allocated objects (e.g. variables), the second one is known as <em>run-time stack</em> (or <strong>stack</strong> for short), it's also known as <em>call stack</em> but we are going to stick to the term run-time stack in our discussions. Please note that the short names of run-time stack (that is, stack) and run-time heap (that is, heap) are also names for <strong>data structures</strong>. As we will see shortly, a data structure describes a way of storing data and how to manipulate this data, while in our current context these two terms are used to represent <strong>memory regions</strong> of a running process although the stack (as memory region) uses stack data structure to store the data. Due to that, here we use the more accurate term <em>run-time stack</em> to refer the memory region and <em>stack</em> to refer the data structure.</p>
<p>Run-time stack is used to store the values of local variables and function parameters, we can say that the run-time stack is a way to implement <em>function's invocation</em> which describes how function <code>A</code> can call function <code>B</code>, pass to it some parameters, return back to the same point of code where function <code>A</code> called function <code>B</code> and finally get the returned value from function <code>B</code>, the implementation details of these steps is known as <em>calling convention</em> and the run-time stack is one way of realizing these steps. There are multiple known calling conventions for x86, different compilers and operating systems may implement different calling conventions, we are not going to cover those different methods but what we need to know that, as we said, those different calling conventions use the run-time stack as a tool to realize function's invocation. The memory region in x86 which is called run-time stack uses a data structure called <em>stack</em> to store the data inside it and to manipulate that data.</p>
<h3 id="the-theory-stack-data-structure"><span class="header-section-number">2.6.1</span> The Theory: Stack Data Structure</h3>
<p>Typically, a <em>data structure</em> as a concept is divided into two components, the first one is the way of storing the data in a high-level terms, a data structure is not concerned about how to store the data in low-level (e.g as bits, or bytes. In the main memory or on the disk, etc.). But it answers the question of storing data as a high-level concept (as we will see in stack example) without specifying the details of implementation and due to that, they are called <em>abstract data structures</em>. The second component of a data structure is the available operations to manipulate the stored data. Of course, the reason of the existence of each data structure is to solve some kind of problem.</p>
<p>In stack data structure, the data will be stored in first-in-last-out (FILO) manner <a href="#fn28" class="footnoteRef" id="fnref28"><sup>28</sup></a>, that is, the first entry which is stored in the stack can be fetched out of the stack at last. The operations of stack data structure are two: <em>push</em> and <em>pop</em> <a href="#fn29" class="footnoteRef" id="fnref29"><sup>29</sup></a>, the first one puts some value on the <em>top of the stack</em>, which means that the top of stack always contains the last value that have been inserted (pushed) into a stack. The latter operation <code>pop</code> <strong>removes</strong> the value which resides on the top of the stack and returns it to the user, that means the most recent value that has been <code>push</code>ed to the stack will be fetched when we <code>pop</code> the stack.</p>
<div id="fig:abcd-stack-step1" class="fignos">
<div class="figure">
<img src="Figures/x86-ch/abcd-stack-step1.png" alt="Figure 6: A Stack with Two Values A and B Pushed Respectively." style="width:35.0%" />
<p class="caption"><span>Figure 6:</span> A Stack with Two Values <code>A</code> and <code>B</code> Pushed Respectively.</p>
</div>
</div>
<p>Let's assume that we have the string <code>ABCD</code> and we would like to push each character separately into the stack. First we start with the operation <code>push A</code> which puts the value <code>A</code> on the top of the stack, then we execute the operation <code>push B</code> which puts the value <code>B</code> on top of the value <code>A</code> as we can see in the figure <a href="#fig:abcd-stack-step1">6</a>, that is, the value <code>B</code> is now on the top of the stack and not the value <code>A</code>, the same is going to happen if we push the value <code>C</code> next as you can see in the figure <a href="#fig:abcd-stack-step2">7</a> and the same for the value of <code>D</code> and you can see the final stack of these four push operations in figure <a href="#fig:abcd-stack-step3">8</a>.</p>
<div id="fig:abcd-stack-step2" class="fignos">
<div class="figure">
<img src="Figures/x86-ch/abcd-stack-step2.png" alt="Figure 7: A Stack with Three Values A, B and C Pushed Respectively." style="width:35.0%" />
<p class="caption"><span>Figure 7:</span> A Stack with Three Values <code>A</code>, <code>B</code> and <code>C</code> Pushed Respectively.</p>
</div>
</div>
<div id="fig:abcd-stack-step3" class="fignos">
<div class="figure">
<img src="Figures/x86-ch/abcd-stack-step3.png" alt="Figure 8: A Stack with Four Values A, B, C and D Pushed Respectively." style="width:35.0%" />
<p class="caption"><span>Figure 8:</span> A Stack with Four Values <code>A</code>, <code>B</code>, <code>C</code> and <code>D</code> Pushed Respectively.</p>
</div>
</div>
<p>Now let's assume that we would like to read the values from this stack, the only way to read data in stack data structure is to use the operation <code>pop</code> which, as we have mentioned, removes the value that resides on the top of the stack and returns it to the user, that is, the stack data structure in contrary of array data structure <a href="#fn30" class="footnoteRef" id="fnref30"><sup>30</sup></a> doesn't have the property of <em>random access</em> to the data, so, if you want to access any data in the stack, you can only use <code>pop</code> to do that. That means if you want to read the first pushed value to the stack, then you need to <code>pop</code> the stack <code>n</code> times, where <code>n</code> is the number of pushed elements into the stack, in other words, the size of the stack.</p>
<p>In our example stack, to be able to read the first pushed value which is <code>A</code> you need to <code>pop</code> the stack four times, the first one removes the value <code>D</code> from the top of stack and returns it to the user, which makes the values <code>C</code> on the top of stack as you can see in figure <a href="#fig:abcd-stack-step2">7</a> and if we execute <code>pop</code> once again, the value <code>C</code> will be removed from the top of the stack and returns it to the user, which makes the value <code>B</code> on the top of the stack as you can see in figure <a href="#fig:abcd-stack-step1">6</a>, so we need to <code>pop</code> the stack two times more the get the first <code>push</code>ed value which is <code>A</code>. This example makes it obvious for us why the stack data structure is described as first-in-last-out data structure.</p>
<p>The stack data structure is one of most basic data structures in computer science and there are many well-known applications that can use stack to solve a specific problem in an easy manner. To take an example of applications that can use a stack to solve a specific problem let's get back to our example of <code>push</code>ing <code>ABCD</code> into a stack, character by character and then <code>pop</code>ping them back, the input is <code>ABCD</code> but the output of <code>pop</code> operation is <code>DCBA</code> which is the reverse string of the input, so, the stack data structure can be used to solve the problem of getting the reversed string of an input by just pushing it into a stack character by character and then popping this stack, concatenating the returned character with the previously returned character, until the stack becomes empty. Other problems that can be solved easily with stack are palindrome problem and parenthesis matching problem which is an important one for a part of programming languages' compilers and interpreters known as parser.</p>
<p>As you can see in this brief explanation of stack data structure, we haven't mention any implementation details which means that a specific data structure is an abstract concept that describes a high-level idea where the low-level details are left for the implementer.</p>
<h3 id="the-implementation-x86-run-time-stack"><span class="header-section-number">2.6.2</span> The Implementation: x86 Run-time Stack</h3>
<p>Now, with our understanding of the theoretical aspect of stack data structure, let's see how the run-time stack is implemented in x86 architecture to be used for the objectives that we have mentioned in the beginning of the subsection. As we have said earlier, the reason of x86 run-time stack's existence is to provide a way to implement function's invocation, that is, the lifecycle of functions. Logically, we know that a program consists of multiple functions (or <em>routines</em> which is another term that is used to describe the same thing) and when executing a program (a process), a number of these functions (not necessarily all of them) should be called to fulfill the required job.</p>
<p>In run-time context, a function <code>B</code> starts its life when it's called by another function <code>A</code>, so, the function <code>A</code> is the <em>caller</em>, that is, the function that originated the call, and the function <code>B</code> is the <em>callee</em>. The caller can pass a bunch of parameters to the callee which can reach the value of these parameters while it's running, the callee can define its own local variables which should not be reached by any other function, that means that these variables can be removed from the memory once the callee finishes its job. When the callee finishes its job, it may <em>return</em> some value to the caller <a href="#fn31" class="footnoteRef" id="fnref31"><sup>31</sup></a>. Finally, the run-time platform (the processor in the case of compiled languages) should be able to know, when the callee finishes, where is the place of the code that should be executed next, and logically, this place is the line in the source code of the caller function which is next to the line that called the callee in the first place.</p>
<p>In x86, each process has its own run-time stack <a href="#fn32" class="footnoteRef" id="fnref32"><sup>32</sup></a>, we can imagine this run-time stack as a big (or even small, that depends on practical factors) memory region that obeys the rules of stack data structure. This run-time stack is divided into multiple mini-stacks, more formally, these mini-stacks are called <em>stack frames</em>. Each stack frame is dedicated to <strong>one</strong> function which has been called during the execution of the program, once this function exists, its frame will be removed from the larger process stack, hence, it will be removed from the memory.</p>
<p>The x86 register <code>EBP</code> (which is called the <em>stack frame base pointer</em>) contains the starting memory address of the current stack frame, and the register <code>ESP</code> (which is called the <em>stack pointer</em>) contains the memory address of the top of the stack. To push a new item into the run-time stack, an x86 instruction named <code>push</code> can be used with the value of the new item as an operand, this instruction decrements the value of <code>ESP</code> to get a new <em>starting</em> memory location to put the new value on and to keep <code>ESP</code> pointing to the top of the stack, decrementing the value of <code>ESP</code> means that the newly pushed items are stored in a lower memory location than the previous value and that means the run-time stack in x86 <em>grows downward</em> in the memory.</p>
<p>When we need to read the value on the top of the stack and removes this value from the stack, the x86 instruction <code>pop</code> can be used which is going to store the value (which resides on the top of stack) on the specified location on its operand, this location can be a register or a memory address, after that, <code>pop</code> operation increments the value of <code>ESP</code>, so the top of stack now refers to the previous value. Note that the <code>pop</code> instruction only increments <code>ESP</code> to get rid of the popped value and don't clear it from memory by, for example, writing zeros on its place which is better for the performance, and this is one of the reasons when you refer to some random memory location, for example in C pointers, and you see some weird value that you probably don't remember that you have stored it in the memory, once upon a time, this value may have been pushed into the run-time stack and its frame has been removed. This same practice is also used in modern filesystems for the sake of performance, when you delete a file the filesystem actually doesn't write zeros in the place of the original content of the file, instead, it just refer to its location as a free space in the disk, and maybe some day this location is used to store another file (or part of it), and this is when the content of the deleted file are actually cleared from the disk.</p>
<p>Let's get back to x86 run-time stack. To make the matter clear in how <code>push</code> and <code>pop</code> work, let's take an example. Assume that the current memory address of the top of stack (<code>ESP</code>) is <code>102d</code> and we executed the instruction <code>push A</code> where <code>A</code> is a character encoded in <code>UTF-16</code> which means its size is <code>2</code> bytes (<code>16</code> bits) and it is represented in hexadecimal as <code>0x0410</code>, by executing this <code>push</code> instruction the processor is going to subtract <code>2</code> from <code>ESP</code> (because we need to push <code>2</code> bytes into the stack) which gives us the new memory location <code>100d</code>, then the processor stores the first byte of <code>UTF-16</code> <code>A</code> (<code>0x04</code>) in the location <code>100d</code> and the second byte (<code>0x10</code>) in the location <code>101d</code> <a href="#fn33" class="footnoteRef" id="fnref33"><sup>33</sup></a>, the value of <code>ESP</code> will be changed to <code>100d</code> which now represents the top of the stack.</p>
<p>When we need to <code>pop</code> the character <code>A</code> from the top of the stack, both bytes should be read and <code>ESP</code> should be <strong>incremented</strong> by <code>2</code>. In this case, the new memory location <code>100d</code> can be considered as a <em>starting</em> location of the data because it doesn't store the whole value of <code>A</code> but a part of it, the case where the new memory location is not considered as starting memory location is when the newly pushed values is pushed as whole in the new memory location, that is, when the size of this value is <code>1</code> byte.</p>
<h3 id="calling-convention"><span class="header-section-number">2.6.3</span> Calling Convention</h3>
<p>When a function <code>A</code> needs to call another function <code>B</code>, then as a first step <code>A</code> (the caller) should push into the stack the parameter that should be passed to <code>B</code> (the callee), that means the parameters of <code>B</code> will be stored on the stack frame of <code>A</code>, when pushing the parameters, they are pushed in a reversed order, that is, the last parameter is pushed first and so on. Then the x86 instruction <code>call</code> can be used to jump to function <code>B</code> code. Before jumping to the callee code, the instruction <code>call</code> pushes the current value of <code>EIP</code> (this is, the returning memory address) onto the stack, at this stage, the value of <code>EIP</code> is the memory address of the instruction of <code>A</code> which is right after <code>call B</code> instruction, pushing this value into the stack is going to help the processor later to decide which instruction of the running code should be executed after the function <code>B</code> finishes. Now, assume that the function <code>B</code> receives three parameters <code>p1</code>, <code>p2</code> and <code>p3</code>, the figure <a href="#fig:call-conv-1">9</a> shows the run-time stack at the stage where <code>call</code> instruction has been performed its first step (pushing <code>EIP</code>). Also, the following assembly code shows how <code>A</code> pushes the parameters then calls <code>B</code>, as you can see, after <code>B</code> finishes and the execution of <code>A</code> resumes, the value of <code>EAX</code> is moved to <code>ECX</code> and this line is just an example and not a part of calling convention.</p>
<div class="sourceCode"><pre class="sourceCode asm"><code class="sourceCode fasm"><span class="fu">A:</span>
<span class="co">; A&#39;s Code Before Calling B</span>

<span class="bu">push</span> p3
<span class="bu">push</span> p2
<span class="bu">push</span> p1
<span class="bu">call</span> B
<span class="bu">mov</span> <span class="kw">ecx</span>, <span class="kw">eax</span>

<span class="co">; Rest of A&#39;s Code</span></code></pre></div>
<div id="fig:call-conv-1" class="fignos">
<div class="figure">
<img src="Figures/x86-ch/call-conv-1.png" alt="Figure 9: Run-time Stack Before Jumping to Function B Code" style="width:55.0%" />
<p class="caption"><span>Figure 9:</span> Run-time Stack Before Jumping to Function <code>B</code> Code</p>
</div>
</div>
<p>When the processor starts executing function <code>B</code>, or any other function, it's the job of the function to create its own stack frame, therefore, the first piece of any function's code should be responsible for creating a new stack frame, this happens by moving the value of <code>ESP</code> (the memory address of the top of stack) to the register <code>EBP</code>, but before that, we should not lose the previous value of <code>EBP</code> (the starting memory address of the caller's stack frame), this value will be needed when the callee <code>B</code> finishes, so, the function <code>B</code> should push the value of <code>EBP</code> onto the stack and only after that it can change <code>EBP</code> to the value of <code>ESP</code> which creates a new stack frame for function <code>B</code>, at this stage, both <code>EBP</code> and <code>ESP</code> points to the top of the stack and the value which is stored in the top of the stack is memory address of the previous <code>EBP</code>, that is, the starting memory location of <code>A</code>'s stack frame. Figure <a href="#fig:call-conv-2">10</a> shows the run-time stack at this stage. The following code shows the initial instructions that a function should perform in order to create a new stack frame as we just described.</p>
<div class="sourceCode"><pre class="sourceCode asm"><code class="sourceCode fasm"><span class="fu">B:</span>
<span class="bu">push</span> <span class="kw">ebp</span>
<span class="bu">mov</span> <span class="kw">ebp</span>, <span class="kw">esp</span>

<span class="co">; Rest of B&#39;s Code</span></code></pre></div>
<div id="fig:call-conv-2" class="fignos">
<div class="figure">
<img src="Figures/x86-ch/call-conv-2.png" alt="Figure 10: Run-time Stack After Jumping to Function B Code and Creating B&#39;s Stack Frame" style="width:55.0%" />
<p class="caption"><span>Figure 10:</span> Run-time Stack After Jumping to Function <code>B</code> Code and Creating <code>B</code>'s Stack Frame</p>
</div>
</div>
<p>Now, the currently running code is function <code>B</code> with its own stack frame which contains nothing. Depending on <code>B</code>'s code, new items can be pushed onto the stack, and as we have said before, the local variables of the function are pushed onto the stack by the function itself, as you know, x86's protected mode is a <code>32-bit</code> environment, so, the values that are pushed onto the stack through the instruction <code>push</code> are of size <code>4</code> bytes (<code>32</code> bits).</p>
<p>Pushing a new item will make the value of <code>ESP</code> to change, but <code>EBP</code> remains the same until the current function finishes its work, this will make <code>EBP</code> too useful when we need to reach the items that are stored in previous function's stack frame (in our case <code>A</code>), for example, the parameters or even the items that are in the current function's stack frame but are not in the top of the stack, as you know, in this case <code>pop</code> cannot be used without losing other values. Instead, <code>EBP</code> can be used as a reference to the other values. Let's take an example of that, given the run-time stack in figure <a href="#fig:call-conv-2">10</a> assume that function <code>B</code> needs to get the value of <code>p1</code>, that can be achieved by reading the memory location of the memory address <code>EBP + 8</code>. As you can see from the figure, memory address of <code>EBP</code> points to the previous value of <code>EBP</code> which its size is <code>4</code> bytes, so if we add <code>4</code> to the value in <code>EBP</code>, that is, <code>EBP + 4</code> we will get the memory address of the location which stores the resume point (<code>EIP</code> before calling <code>B</code>) which also has the size of <code>4</code> bytes, so, if we add another <code>4</code> bytes to <code>EBP</code> we will reach the item which is above the resume point, which will always (because the convention always work the same way with any function) be the first parameter if the current function receives parameters, and by adding another <code>4</code> to <code>EBP</code> we will get the second parameter and so on. The same is applicable if we would like to read values in current function's stack frame (e.g. local variables), but this time we need to subtract from <code>EBP</code> instead of adding to it. Whether we are adding to or subtracting from <code>EBP</code> the value will always be <code>4</code> and its multiples since each item in x86 protected-mode run-time stack is of <code>4</code> bytes. The following assembly example of <code>B</code> reads multiple values from the stack that cannot be read with normal <code>pop</code> without distorting the stack.</p>
<div class="sourceCode"><pre class="sourceCode asm"><code class="sourceCode fasm"><span class="fu">B:</span>
<span class="co">; Creating new Stack Frame</span>
<span class="bu">push</span> <span class="kw">ebp</span>
<span class="bu">mov</span> <span class="kw">ebp</span>, <span class="kw">esp</span>

<span class="bu">push</span> <span class="dv">1</span> <span class="co">; Pushing a local variable</span>
<span class="bu">push</span> <span class="dv">2</span> <span class="co">; Pushing another local variable</span>

<span class="co">; Reading the content </span>
<span class="co">; of memory address EBP + 4</span>
<span class="co">; which stores the value of</span>
<span class="co">; the parameters p1 and moving</span>
<span class="co">; it to eax.</span>
<span class="bu">mov</span> <span class="kw">eax</span>, [<span class="kw">ebp</span> + <span class="dv">8</span>]

<span class="co">; Reading the value of the</span>
<span class="co">; first local varaible and</span>
<span class="co">; moving it to ebx.</span>
<span class="bu">mov</span> <span class="kw">ebx</span>, [<span class="kw">ebp</span> - <span class="dv">4</span>]

<span class="co">; Rest of B&#39;s Code</span></code></pre></div>
<p>When <code>B</code> finishes and needs to return a value, this value should be stored in the register <code>EAX</code>. After that, <code>B</code> should deallocates its own stack frame, this task can be accomplished easily by popping all values of <code>B</code>'s stack frame until we reach to first value pushed value by <code>B</code> (the starting memory address of the caller <code>A</code> stack frame) which should be set to <code>EBP</code> in order to restore the stack frame of <code>A</code> as the current stack frame. After that, the top of the stack contains the returning memory address which should be loaded to <code>EIP</code> so we can resume the execution of the caller <code>A</code>, that's can be done by using the x86 instruction <code>ret</code> which pops the stack to get the returning address then loads <code>EIP</code> with this value. Finally, when <code>A</code> gains the control again it can deallocate the parameters of <code>B</code> to save some memory by just popping them. The method that we have described to deallocate the whole stack frame or deallocate the parameters is the standard way that's not widely used practically for multiple reasons, one of these reasons is that <code>pop</code> needs a place to store the <code>pop</code>ped value, this place can be a register or a memory location, but what we really need is to get rid of these values, so, storing them in another place is a waste of memory. In order to explain the other way of deallocating some items from the stack consider the following code:</p>
<div class="sourceCode"><pre class="sourceCode asm"><code class="sourceCode fasm"><span class="bu">sub</span> <span class="kw">esp</span>, <span class="dv">4</span>
<span class="bu">mov</span> [<span class="kw">esp</span>], <span class="dv">539</span></code></pre></div>
<p>This code is equivalent to <code>push 539</code>, it does exactly what <code>push</code> does, first it subtract <code>4</code> bytes from top of stack's memory address to get a new memory location to store the new value in, then, it stores the value in this location. The reverse operation is performed with <code>pop</code> as the following which is equivalent to <code>pop eax</code>.</p>
<div class="sourceCode"><pre class="sourceCode asm"><code class="sourceCode fasm"><span class="bu">mov</span> <span class="kw">eax</span>, [<span class="kw">esp</span>]
<span class="bu">add</span> <span class="kw">esp</span>, <span class="dv">4</span></code></pre></div>
<p>As you can see, to get rid of the <code>pop</code>ped value, only top of stack's memory address has been changed. Since every item on the stack is of size <code>4</code> bytes, then adding <code>4</code> to <code>ESP</code> makes it point to the item which is exactly above the current one in the stack. So, if we need to get rid of the value on the top of stack without getting its value and storing it somewhere else, we can simply use the following instruction <code>add esp, 4</code>. What if we want to get rid of the value on the top of the stack and the value before it? The total size of both of them is <code>8</code> bytes, so, <code>add esp, 8</code> will do what we want. This technique is applicable for both deallocating <code>B</code>'s stack frame and its parameters from <code>A</code>'s stack frame. For the former, there is a yet better technique. In order to deallocate the stack frame of the current function we can simply do the following: <code>mov esp, ebp</code>, that is, move the memory address of <code>EBP</code> to <code>ESP</code>, which was the state when the callee <code>B</code> just started. The following is the last part of <code>B</code> which deallocates its own stack frame and return to <code>A</code>.</p>
<div class="sourceCode"><pre class="sourceCode asm"><code class="sourceCode fasm"><span class="fu">B:</span>
<span class="co">; Previous B&#39;s Code:</span>
<span class="co">;        Creating new Stack Frame</span>
<span class="co">;        Pushing Local variable</span>
<span class="co">;        The Rest of Code</span>

<span class="co">; Make to the of stack points</span>
<span class="co">; to the first value pushed</span>
<span class="co">; by the function &quot;B&quot;.</span>
<span class="bu">mov</span> <span class="kw">esp</span>, <span class="kw">ebp</span>

<span class="co">; Pop the current top of</span>
<span class="co">; stack and put the value</span>
<span class="co">; in &quot;EBP&quot; to make A&#39;s</span>
<span class="co">; stack frame as the current.</span>
<span class="bu">pop</span> <span class="kw">ebp</span>

<span class="co">; Jump the the resume point.</span>
<span class="bu">ret</span></code></pre></div>
<p>The details of calling a function that we have just described are <strong>implementation details</strong> and we mentioned previously that these implementation details of function's invocation are known as calling conventions. The calling convention that we have described is known as <code>cdecl</code> which stands for <em>C declaration</em>, there are other conventions, which means the one which we have described is not an strict standard for x86 architecture, instead, the operating systems, compilers and low-level code writers can decide which calling convention that they would like to use or maybe make up a wholly new one according to their objective. However, the reason behind choosing <code>cdecl</code> to explain here is that it is a well-known and widely used calling convention, also, it serves our purpose of explaining the basics of x86 run-time stack.</p>
<h3 id="growth-direction-of-run-time-stack"><span class="header-section-number">2.6.4</span> Growth Direction of Run-time Stack</h3>
<p>When we explained how x86 instructions <code>push</code> and <code>pop</code> work, we have claimed that the x86 run-time stack <em>grows downward</em>, so, what does growing downward or upward exactly means? Simply, when we said that x86 run-time stack grows downward we meant the the older items of stack are pushed on larger memory addresses while the most recent ones are pushed onto smaller memory addresses. For example, starting from the memory address <code>104d</code>, let's assume we have pushed the value <code>A</code> after that we pushed the value <code>B</code>, then <code>A</code>'s memory location will be <code>104d</code> while <code>B</code>'s memory location will be <code>100d</code>, so the new values will always be pushed on the bottom of the old ones in the memory.</p>
<div id="fig:10062021_0" class="fignos">
<div class="figure">
<img src="Figures/x86-ch/Fig10062021_0.png" alt="Figure 11: An Example of a Run-Time Stack with Three Items" style="width:35.0%" />
<p class="caption"><span>Figure 11:</span> An Example of a Run-Time Stack with Three Items</p>
</div>
</div>
<p>What makes we claim that, for instance, the address <code>100d</code> is at the bottom of <code>104d</code> instead of the other way around is how we visualize the run-time stack inside the main memory. Let's look at the figure <a href="#fig:10062021_0">11</a> which shows a run-time stack that contains three items <code>M</code>, <code>R</code> and <code>A</code> and all of them are of <code>4</code> bytes, on the right side of the figure we can see the starting memory address of each item. As we can see, in this visualization of the run-time stack, the smaller memory addresses are on the bottom and the larger memory addresses are on the top.</p>
<p>From the figure we can see that the value of <code>ESP</code> is <code>8d</code> <a href="#fn34" class="footnoteRef" id="fnref34"><sup>34</sup></a>, let's assume that we would like to run the instruction <code>push C</code> on this run-time stack, as we have mentioned before, the instruction <code>push</code> of x86 is going to decrease the value of <code>ESP</code> by a size decided by the architecture (<code>4</code> bytes in our case) in order to get a new starting memory address for the new item. So, <code>push</code> is going to subtract <code>4d</code> (The size of pushed item <code>C</code> in bytes) from <code>8d</code> (current <code>ESP</code> value) which gives us the new starting memory location <code>4d</code> for the item <code>C</code>. If we visualize the run-time stack after pushing <code>C</code> it will be the one as on figure <a href="#fig:10062021_1">12</a> and we can see, depending on the way of <code>push</code> instruction works, that the stack grew downwards by pushing the new item <code>C</code> on the bottom. So, according to this visualization of run-time stack, which puts larger memory addresses on the top and smaller on the bottom, we can say x86 run-time stack grows downward <em>by default</em>.</p>
<div id="fig:10062021_1" class="fignos">
<div class="figure">
<img src="Figures/x86-ch/Fig10062021_1.png" alt="Figure 12: A New Item Pushed Into a Stack that Grows Downward" style="width:35.0%" />
<p class="caption"><span>Figure 12:</span> A New Item Pushed Into a Stack that Grows Downward</p>
</div>
</div>
<p>This visualization is just one way to view how the run-time stack grows, which means they may be other visualizations, and the most obvious one is to reverse the one that we just described by putting the smaller addresses on the top and the larger addresses on the bottom as shown in figure <a href="#fig:10062021_2">13</a>, you can note that in contrast to figure <a href="#fig:10062021_1">12</a> the smallest address <code>4d</code> is on top, so, based on this visualization the stack grows upward! Actually this latter visualization of run-time stack is the one which is used in Intel's manual and the term <em>expand-up</em> is the term that is used in the manual to describe the direction of stack growth.</p>
<div id="fig:10062021_2" class="fignos">
<div class="figure">
<img src="Figures/x86-ch/Fig10062021_2.png" alt="Figure 13: A Stack that Grows Upward Instead of Downward" style="width:35.0%" />
<p class="caption"><span>Figure 13:</span> A Stack that Grows Upward Instead of Downward</p>
</div>
</div>
<p>To sum it up, the direction in which the run-time stack grows (down or up) depends on how do you visualize the run-time stack, as in figure <a href="#fig:10062021_1">12</a> or as in figure <a href="#fig:10062021_2">13</a>. In our discussion in this book we are going to depend on the first visualization<a href="#fn35" class="footnoteRef" id="fnref35"><sup>35</sup></a>, so, simply, the run-time stack of x86 grows downward.</p>
<h3 id="the-problem-of-resizing-the-run-time-stack"><span class="header-section-number">2.6.5</span> The Problem of Resizing the Run-time Stack</h3>
<p>We have emphasized that x86 run-time stack <strong>by default</strong> grows downward, this default behavior can be changed if we wish to, which is going to make the run-time stack to grow upwards instead and the way to do that is to use expansion-direction flag of run-time stack's segment descriptor, we have mentioned this flag when explained the structure of segment descriptor and postponed its details till here.</p>
<p>When we want the run-time stack to grow downward (or in Intel's term which depends on the second visualization of run-time stack: <strong>expand-up</strong>) the value of this flag should be <code>0</code>, on the other hand, when we want the run-time stack to grow upward (in Intel's term: <strong>expand-down</strong>) the value of this flag should be <code>1</code>. Modern operating systems use the default behavior (downward growth), we will see that this design decision is taken due to the choice of flat memory model by modern operating systems. However, the other available option (upward growth) is there to solve a potential problem and whether this problem is going to show up in a specific kernel depends on how this kernel's architecture is designed, that is, which memory model is used in this kernel.</p>
<p>This problem, which we can solve by making the run-time stack grows upward instead of downward, is related to the need of increasing the size of run-time stack and the fact that the run-time stack stores memory addresses <a href="#fn36" class="footnoteRef" id="fnref36"><sup>36</sup></a> on it. Let's assume that our kernel created a new stack segment for a specific process <code>X</code> and this stack segment has a fixed size which is <code>50</code> bytes <a href="#fn37" class="footnoteRef" id="fnref37"><sup>37</sup></a> for example. The process <code>X</code> starts its work and at some point of time its run-time stack of size <code>50</code> bytes becomes full which means that we need to resize it and make it bigger so it can hold more items, let's assume the new size will be <code>60</code> bytes.</p>
<div id="fig:10062021_3" class="fignos">
<div class="figure">
<img src="Figures/x86-ch/Fig10062021_3.png" alt="Figure 14: Process X&#39;s Run-time Stack" style="width:35.0%" />
<p class="caption"><span>Figure 14:</span> Process X's Run-time Stack</p>
</div>
</div>
<p>Before going any further with our discussion, let's see the figure <a href="#fig:10062021_3">14</a> which represents a snapshot of process <code>X</code>'s run-time stack when it became full. We can see from the figure that <code>X</code>'s stack segment starts from the <strong>physical</strong> memory address <code>300d</code> (segment's base address) and ends at the <strong>physical</strong> memory address <code>250d</code>, also, the items of run-time stack are referred to based on their offsets inside the stack segment. We can see that a bunch of values have been pushed onto the stack, some of those values are shown on the figure and some other are omitted and replaced by dots which means that there are more values here in those locations. Normal values are called &quot;some value&quot; in the figure and the last pushed value in the stack is the value <code>Z</code>. Also, a value which represents a <strong>logical memory address</strong> has been pushed onto the stack, more accurately, this value represents an <strong>offset</strong> within the current stack segment, a <strong>full</strong> logical memory address actually consists of both offset <strong>and</strong> segment selector as we have explained earlier in this chapter when we discussed address translation. But for the sake of simplicity, we are going call this stored value as &quot;memory address&quot; or &quot;memory location&quot; in our current explanation. As we explained earlier, all memory addresses that the processes work with are logical and not physical. The value <code>26d</code> is a local variable <code>P</code> of the type pointer (as in C) which points to another local variable <code>R</code> that has the value <code>W</code> and is stored in the memory location <code>26d</code>.</p>
<div id="fig:10062021_4" class="fignos">
<div class="figure">
<img src="Figures/x86-ch/Fig10062021_4.png" alt="Figure 15: Process X&#39;s Run-time Stack After Resize (Grows Downward)" style="width:35.0%" />
<p class="caption"><span>Figure 15:</span> Process X's Run-time Stack After Resize (Grows Downward)</p>
</div>
</div>
<p>Figure <a href="#fig:10062021_4">15</a> shows <code>X</code>'s stack after resize, as you can see we have got our new free space of <code>10</code> bytes, also, because the stack grows downward so the new free space should be added on the bottom of the stack to be useful which means the previous offsets should be changed, therefore, the largest offset <code>50d</code> has been updated to <code>60d</code> by adding <code>10d</code> (which is the newly added free space to the stack in bytes) to it and so on for the rest of offsets, also, <code>ESP</code> has been simply updated in the same manner.</p>
<p>Now we can see that the process of updating the offsets, that we are forced to perform because the stack grows downward, has caused a problem in the offsets which have been pushed onto the stack before resizing it. You can see the pointer <code>P</code> which still has the original value <code>26d</code>, that means it doesn't point the the variable <code>R</code> anymore, instead it is going to point to another memory location now with a value other than <code>W</code>, and the same problem holds for all pushed <code>EBP</code> values on <code>X</code>'s stack.</p>
<p>A potential solution for this problem is to update all stack items that contain memory addresses in the range of the stack after resizing it, exactly as we have done with <code>EBP</code>, but more simpler solution is to make the stack to grow upwards instead of downwards! Modern operating systems solves this problem by not dividing the memory into segments but they use flat memory model which views the memory as one big segment for all data, code and stacks.</p>
<div id="fig:12062021_1" class="fignos">
<div class="figure">
<img src="Figures/x86-ch/Fig12062021_1.png" alt="Figure 16: Process X&#39;s Run-time Stack (Grows Upward)" style="width:35.0%" />
<p class="caption"><span>Figure 16:</span> Process X's Run-time Stack (Grows Upward)</p>
</div>
</div>
<p>Now let's see what happens in the same scenario but with changing the growth direction of the stack from downward which caused the problem to upwards. In this case, as we have said before, the new items will be stored on the larger memory addresses (offsets to be more accurate). Figure <a href="#fig:12062021_1">16</a> shows the same snapshot of <code>X</code>'s run-time of stack as in the one of figure <a href="#fig:10062021_3">14</a> but this time it grows upwards instead of downward. You can notice that the older values are now on the bottom of the stack, that is, on smaller memory addresses, what interests us in this stack is the entry which stores the memory address <code>20d</code> that points to the memory location which has the value <code>W</code> and it is the one which caused the problem in the first place. When the stack was growing downward, the memory location of the value <code>W</code> was <code>26d</code>, but this time it is <code>20d</code>. So, what happens when we need to resize this run-time stack?</p>
<p>In the same way of the previous one, the limit of the stack (its largest offset) will be increased from <code>50d</code> to <code>60d</code> as shown in figure <a href="#fig:12062021_2">17</a>, but in contrast to the previous one, we don't need to update the value of <code>ESP</code> anymore, because as you can see from the two figures <a href="#fig:12062021_1">16</a> and <a href="#fig:12062021_2">17</a> the memory address <code>50d</code> represents the top of the stack on both stacks. The same holds true for the stack item which stores the memory address <code>20d</code>, we don't need to update it because the value <code>W</code> is still on the same memory address (offset) and can be pointed to by the memory address <code>20d</code>. So, we can say that deciding the direction of run-time stack growth to be upward instead of downward can easily solve the problem of getting wrong stored memory address after resizing the run-time stack <a href="#fn38" class="footnoteRef" id="fnref38"><sup>38</sup></a> and that's when we use segmentation as a way of viewing the memory.</p>
<div id="fig:12062021_2" class="fignos">
<div class="figure">
<img src="Figures/x86-ch/Fig12062021_2.png" alt="Figure 17: Process X&#39;s Run-time Stack (Grows Upward) Resized" style="width:35.0%" />
<p class="caption"><span>Figure 17:</span> Process X's Run-time Stack (Grows Upward) Resized</p>
</div>
</div>
<h2 id="x86-interrupts"><span class="header-section-number">2.7</span> x86 Interrupts</h2>
<p>Event-driven programming is a common programming paradigm that is used in many areas of programming. One of these areas is graphical user interface (GUI) programming, also, it is common in game development, furthermore, some network programming frameworks use this paradigm. In this paradigm, the program is driven by <em>events</em>, that is, it keeps idle and waiting for any even to occur and once an event occurs the program starts to work by handling this event, for example, a mouse click is considered as an event in GUI programming. When an event occurs, the program handles this event through, usually, a separated function which is dedicated for this event, this function is known as a <em>handler</em>. In GUI programming for example, when the user clicks on a specific button, that is, when this event occurs, a function specified for this event on this button (the handler) is called to perform some operation after this click, such as, save a document or close the application.</p>
<p>This paradigm is also used by x86. When a process is running, something can <em>interrupt</em> (an event occurred) the processor which is going, in this case, to stop the execution of the current process temporarily, and call the suitable <em>interrupt handler</em> (also called <em>interrupt service routine</em>) to handle the current interrupt, after handling the interrupt, the processor can resume the process which was running before the interrupt occurred.</p>
<p>One example of the usage of interrupts in this low-level environment is the <em>system timer</em>. In the hardware level, there could be a system timer which interrupts the processor in each <code>X</code> period of time and this type of interrupt is the one that makes multitasking possible in uniprocessor systems. When a processor is interrupted by the system timer, it can call the kernel which can change the currently running process to another one; this operation known as <em>scheduling</em> which its goal is distributing the time of the processor to the multiple processes in the system.</p>
<p>Another example of using interrupts is when the user of an operating system presses some keys on the keyboard, these events of pressing keyboard keys should be sent to the kernel which is going to delegate the <em>device driver</em> <a href="#fn39" class="footnoteRef" id="fnref39"><sup>39</sup></a> of the keyboard to handle these events in a proper way, in this case, with each key press, the keyboard is going to interrupt the processor and request to handle these events.</p>
<p>In x86, both hardware and software can interrupt the processor, system timer and keyboard are examples of <em>hardware interrupts</em> while the <em>software interrupt</em> can occur by using the x86 instruction <code>int</code> which we have used when we wrote our bootloader, the operand of this instruction is the <em>interrupt number</em>, for example, in our bootloader we have used the following line <code>int 10h</code>, in this case, the interrupt number is <code>10h</code> (<code>16d</code>) and when the processor is interrupted by this instruction, it is going to call the handler of interrupt number <code>10h</code>. Software interrupt can be used to implement what is known as <em>system calls</em> which provide a way for user applications to call a specific kernel's code that gives the applications some important services such as manipulating the filesystem (e.g. reading or writing files, create new file or directories, etc.) or creating new process and so on in a way that resembles the one that we used to call BIOS services.</p>
<p>In addition to interrupts, <em>exceptions</em> can be considered as another type of events which also stop the processor from its current job temporarily and make it handle it and then resume its job after that. The main difference between exceptions and interrupts in x86 is that the former occurs when an error happens in the environment, for example, when some code tries to divide some number by zero, an exception will be generated and some handler should do something about it, we can perceive the exceptions of x86 as the exceptions of some programming languages such as C++ and Java.</p>
<h3 id="interrupt-descriptor-table"><span class="header-section-number">2.7.1</span> Interrupt Descriptor Table</h3>
<p>In x86, there is a table known as <em>interrupt descriptor table</em> (<code>IDT</code>), also may called <em>interrupt vector table</em> but the term that Intel uses is the former one while the latter are used to describe this kind of tables as a concept in some works of literature and not the name of the table on a specific architecture. <code>IDT</code> resides in the memory and tells the processor how to reach to the handler of a given interrupt number. The entries of <code>IDT</code> are known as <em>gate descriptors</em> and the size of each one of them is <code>8</code> bytes same as <code>GDT</code> and <code>LDT</code>. At most, <code>IDT</code> can contain <code>256</code> gate descriptors and the base memory address of <code>IDT</code> is stored in a special register known as <code>IDTR</code>.</p>
<div id="fig:210621_0" class="fignos">
<div class="figure">
<img src="Figures/x86-ch/Fig210621_0.png" alt="Figure 18: Gate Descriptor Structure for Interrupt and Trap Gates" style="width:50.0%" />
<p class="caption"><span>Figure 18:</span> Gate Descriptor Structure for Interrupt and Trap Gates</p>
</div>
</div>
<p>The gate descriptors in the <code>IDT</code> table can be of three types, <em>task gate</em>, <em>interrupt gate</em> and <em>trap gate</em>, our focus currently will be on the latter two. The structure of both interrupt and trap gate descriptor is shown in figure <a href="#fig:210621_0">18</a>. As we have said earlier, a gate descriptor of the <code>IDT</code> should point to the memory address of the interrupt handler's code. We can see in the figure that bytes <code>2</code> and <code>3</code> should contain a segment selector, which is the segment selector of handler's code, that is, the index of the code segment that contains handler's code, we can see an important difference between <code>GDT</code> and <code>IDT</code> here. In the former the base address of a segment is a linear address, while the base address of the handler is a logical address.</p>
<p>The offset of the first handler's instruction should be set in the descriptor, this will be useful if the handler's code is just a part of the whole code segment which is presented in the segment selector field. The offset in the gate descriptor is divided into two parts, the least significant<code>2</code> bytes of the offset should be loaded into bytes <code>0</code> and <code>1</code> of the descriptor, while the most significant<code>2</code> bytes of the offset should be loaded into bytes <code>6</code> and <code>7</code>.</p>
<p>The least significant nibble of byte <code>4</code> is reserved and the most significant nibble of byte <code>4</code> should always be <code>0d</code>. The most significant bit of byte <code>5</code> is present flag (<code>P</code> flag), when its value is <code>0</code> that means the code that this descriptor is pointing to is not loaded into memory, while the value <code>1</code> means otherwise. The descriptor privilege level field (<code>DPL</code>) contains the privilege level of the handler, it occupies the second and third least significant bit of byte <code>5</code>. The value of fourth, sixth and seventh least significant bits of byte <code>5</code> should always be <code>0b</code>, <code>1b</code> and <code>1b</code> respectively. The flag which is called <code>D</code> in the figure specifies the size of the gate descriptor itself whether it is <code>32</code> bits, when<code>D</code> flag = <code>1</code>, or <code>16</code> bits when <code>D</code> flag = <code>0</code>, the former should always be our choice in protected-mode, while the latter should always be our choice in real-mode. The flag which is called <code>T</code> in the figure specifies whether the gate is an interrupt gate, when <code>T</code> flag = <code>0</code>, or the gate is an trap gate, when <code>T</code> flag = <code>1</code>.</p>
<p>The difference between interrupt and trap gates is too simple, when a handler defined as an interrupt gate is called, the processor is going to disable the ability to signal a new interrupt until the handler returns, that is, the execution of the handler will not interrupted until it finishes its job and return to the caller, of course there are some exceptions, a type of interrupts known as <em>non-maskable interrupts</em> (<code>NMI</code>) will interrupt the execution of the current code even if the interruption is disabled, non-maskable interrupts occur when some catastrophic event (from the hardware perspective) happens in the system. On the other hand, the handler that is defined as a trap gate can be interrupted by any new interrupt, that is, the interruption will not be disabled by the processor.</p>
<p>However, disabling interruption is an operation that can be performed by the code by using the x86 instruction <code>cli</code> (still, non-maskable interrupts are excepted) which stands for <em>clear interrupt flag</em> and can be enabled again by using the instruction <code>sti</code> which stands for <em>set interrupt flag</em>, both of these instructions manipulate the value of <em>interrupt flag</em> which is a part of the register <code>EFLAGS</code>.</p>
<p>Now, let's assume that we have defined a gate descriptor for a handler, let's name it <code>A</code>. The question is, which interrupt the handler <code>A</code> is going to handle? In other words, for which interrupt number the processor is going to call the code of <code>A</code> to handle the interrupt? In fact, that depends on the index of <code>A</code>'s gate descriptor in the <code>IDT</code> table. Let's assume that the index is <code>0d</code>, then <code>A</code>'s code will be called when the interrupt number <code>0</code> is signaled, that means the term interrupt number is a synonym for entry's index number in <code>IDT</code> table</p>
<p>In protected-mode, interrupt numbers, that is <code>IDT</code> entries indices, from <code>0</code> to <code>21</code> have specific meaning defined by x86 architecture itself, for example, the interrupt number that is reserved for division by zero exception is the interrupt number <code>0</code> and, in our example, the code of <code>A</code> will be called when some other code divides a number by zero. Beside interrupt numbers <code>0</code> to <code>21</code>, the range of interrupts number from <code>22</code> to <code>31</code> are reserved, and the interrupt numbers from <code>32</code> to <code>255</code> are available for the system programmer to decide their meanings, however, not all of their descriptors should be filled, only the needed ones will be enough.</p>
<h4 id="the-register-idtr"><span class="header-section-number">2.7.1.1</span> The Register <code>IDTR</code></h4>
<p>In same way as <code>GDT</code>, we should tell the processor where the <code>IDT</code> reside in the memory and that can be performed by the instruction <code>lidt</code> which stands for <em>l</em>oad <code>IDT</code>, this instructions works as <code>lgdt</code>, it takes an operand and loads it to the register <code>IDTR</code> which will be used later by the processor to reach to the <code>IDT</code> table.</p>
<p>The structure of <code>IDTR</code> is same as <code>GDTR</code>, its size is <code>48</code> bits and it's divided into two parts, the first part represents the size of the <code>IDT</code> in bytes, that is, the <code>IDT</code>'s limit, this field starts from bit <code>0</code> of <code>IDTR</code> and ends at bit <code>15</code>. Starting from bit <code>16</code> up to bit <code>47</code> the base linear address <a href="#fn40" class="footnoteRef" id="fnref40"><sup>40</sup></a> where <code>IDT</code> is reside should be set.</p>
<div class="footnotes">
<hr />
<ol>
<li id="fn1"><p>If some of these terms are new for you don't worry about them too much, you will learn them gradually throughout this book.<a href="#fnref1">↩</a></p></li>
<li id="fn2"><p>That is, the user of the operating system can only run one process at a given time. DOS is a an example of monotasking operating system.<a href="#fnref2">↩</a></p></li>
<li id="fn3"><p>Segmentation will be examined in details later on this chapter.<a href="#fnref3">↩</a></p></li>
<li id="fn4"><p>That is, the processes which runs on privilege level greater than <code>0</code><a href="#fnref4">↩</a></p></li>
<li id="fn5"><p>For example, loading <code>GDT</code> register by using the instruction <code>lgdt</code> as we will see later in this chapter.<a href="#fnref5">↩</a></p></li>
<li id="fn6"><p>And from here came the well-known joke: &quot;There are 10 types of people in this world, those who understand binary and those who don't&quot;.<a href="#fnref6">↩</a></p></li>
<li id="fn7"><p>I think It's too brave to state this claim, however, it holds true at least for the well-known numbering system.<a href="#fnref7">↩</a></p></li>
<li id="fn8"><p>Can you tell why? [Hint: How the maximum hexadecimal number <code>F</code> is represented in binary?]<a href="#fnref8">↩</a></p></li>
<li id="fn9"><p>The architecture which each memory address points to <code>1 byte</code> is known as <em>byte-addressable architecture</em> or <em>byte machines</em>. It is the most common architecture. Of course, other architectures are possible, such as <em>word-addressable architecture</em> or <em>word machines</em>.<a href="#fnref9">↩</a></p></li>
<li id="fn10"><p>The concept and term of offset is not exclusive to segmentation, it is used on other topics related to the memory.<a href="#fnref10">↩</a></p></li>
<li id="fn11"><p>Yes, all segments can be on the same memory location, that is, there is a <code>64KB</code> segment of memory which is considered as the currently active code segment, data segment and stack segment. We have already mentioned that when we have discussed how to implement flat-memory model on x86.<a href="#fnref11">↩</a></p></li>
<li id="fn12"><p>Which loads the physical memory address of <code>title_string</code> to the register <code>si</code>.<a href="#fnref12">↩</a></p></li>
<li id="fn13"><p>This is a <strong>relaxed</strong> definition of segment selector, a more accurate one will be presented later.<a href="#fnref13">↩</a></p></li>
<li id="fn14"><p>In real mode, the starting address of the segment is stored directly on the corresponding segment register (e.g. <code>CS</code> for code segment).<a href="#fnref14">↩</a></p></li>
<li id="fn15"><p>Reminder: In protected mode, the corresponding segment register stores the selector of the currently active segment.<a href="#fnref15">↩</a></p></li>
<li id="fn16"><p>And it <strong>should</strong>, since segmentation is enabled by default in x86 and cannot be disabled.<a href="#fnref16">↩</a></p></li>
<li id="fn17"><p>Remember our discussion of the difference between our logical view of the memory (e.g. segmentation) and the actual physical hardware<a href="#fnref17">↩</a></p></li>
<li id="fn18"><p>We can see here how obvious the mapping between the logical view of the memory and the real-world memory.<a href="#fnref18">↩</a></p></li>
<li id="fn19"><p>Don't worry about paging right now. It will be discussed later in this book. All you need to know now is that paging is another logical view of the memory. Paging is disabled by default in x86 which makes it an optional feature unlike segmentation.<a href="#fnref19">↩</a></p></li>
<li id="fn20"><p>Which means <strong>do</strong> enable read, since <code>1</code> is equivalent to <code>true</code> in the context of flags.<a href="#fnref20">↩</a></p></li>
<li id="fn21"><p>Which means <strong>don't</strong> enable read.<a href="#fnref21">↩</a></p></li>
<li id="fn22"><p>Which makes sense, enabling reads from a code segment means it contains data also.<a href="#fnref22">↩</a></p></li>
<li id="fn23"><p>We use the term <em>access</em> here for both types of application segments. While this term is valid for data segment, we mean <em>execute</em> for code segment.<a href="#fnref23">↩</a></p></li>
<li id="fn24"><p>The processor knows it is a stack segment if the segment selector is loaded into stack segment selector register <code>SS</code><a href="#fnref24">↩</a></p></li>
<li id="fn25"><p>In terms of Intel's manual: <em>compatibility mode</em>.<a href="#fnref25">↩</a></p></li>
<li id="fn26"><p>More accurately, the linear address. Refer to discussion of memory translation process in this chapter.<a href="#fnref26">↩</a></p></li>
<li id="fn27"><p>More accurate definition of a segment selector and its structure in protected-mode is presented in the next subsection.<a href="#fnref27">↩</a></p></li>
<li id="fn28"><p>On contrary, <em>queue data structure</em> stores data in first-in-<strong>first</strong>-out (FIFO) manner.<a href="#fnref28">↩</a></p></li>
<li id="fn29"><p>That doesn't mean no more operations can be defined for a given data structure in the implementation level. It only means that the conceptual perspective for a given data structure defines those basic operations which reflect the spirit of that data structure. Remember that when we start to use x86 run-time stack with more operations than <code>push</code> and <code>pop</code> later, though those other operations are not canonical to the stack data structure, but they can be available if the use case requires that (and yes they may violate the spirit of the given data structure! We will see that later).<a href="#fnref29">↩</a></p></li>
<li id="fn30"><p>Which is implemented by default in most major programming languages and know as arrays (in C for example) or lists (as in Python)<a href="#fnref30">↩</a></p></li>
<li id="fn31"><p>Some programming languages, especially those which are derived from Algol differentiate between a <em>function</em> which <strong>should</strong> return a value to the caller, and a <em>procedure</em> which <strong>shouldn't</strong> return a value to the caller.<a href="#fnref31">↩</a></p></li>
<li id="fn32"><p>We claim that for the purpose of explanation. But actually the matter of separated run-time stack for each process is a design decision that the operating system's kernel programmer/designer is responsible for.<a href="#fnref32">↩</a></p></li>
<li id="fn33"><p>In fact, x86 is little-endian architecture which means that <code>0x10</code> will be stored in the location <code>100d</code> while <code>0x04</code> will be stored in the location <code>101d</code> but I've kept the example in the main text as is for the sake of simplicity.<a href="#fnref33">↩</a></p></li>
<li id="fn34"><p>As a reminder, don't forget that all these memory address are actually <strong>offsets</strong> inside a stack segment and not a whole memory address.<a href="#fnref34">↩</a></p></li>
<li id="fn35"><p>And many other books actually uses the first visualization as I recall and for that I chose it in this book. And according to my best knowledge the only reference that I've seen that depends on the second visualization is Intel's manual.<a href="#fnref35">↩</a></p></li>
<li id="fn36"><p>The previous values of <code>EBP</code> and <code>EIP</code>. Also the application programmer may store memory addresses of local variables in the stack (e.g. by using pointers in C).<a href="#fnref36">↩</a></p></li>
<li id="fn37"><p>As you may recall, the size of the segment can be decided by the base address of the segment and its limit as specified in the segment's descriptor.<a href="#fnref37">↩</a></p></li>
<li id="fn38"><p>Actually, the well-know stack overflow vulnerability in x86 is also caused by stack growing downward and can be avoided easily in growing upwards stacks!<a href="#fnref38">↩</a></p></li>
<li id="fn39"><p>That's why in some kernel's designs, especially, monolithic kernel keeps the device drivers as a part of the kernel.<a href="#fnref39">↩</a></p></li>
<li id="fn40"><p>As we have mentioned multiple time that in our current case, where the paging is disabled, a linear address is same as physical address.<a href="#fnref40">↩</a></p></li>
</ol>
</div>
            </div>
    </div>
  </div>
  
  <script src="https://vjs.zencdn.net/5.4.4/video.js"></script>

</body>
</html>
